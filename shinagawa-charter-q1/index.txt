1:"$Sreact.fragment"
2:I[8708,["315","static/chunks/315-19ca0205aa2fdd55.js","458","static/chunks/458-4db2abcd82f4c9c6.js","124","static/chunks/124-8b6d8ff5759d10e8.js","48","static/chunks/48-3a76f8478d544215.js","177","static/chunks/app/layout-4ee75bbda26c9e8d.js"],"Provider"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[32176,["315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","335","static/chunks/app/%5Bslug%5D/error-b85ff2b7d7b78a5a.js"],"default"]
6:I[6874,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],""]
7:I[38567,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Button"]
9:I[59665,[],"OutletBoundary"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/_next/static/css/a63443551c7d7d9f.css","style"]
0:{"P":null,"b":"_waQAkgUjaI7YsOsN9MF2","p":"","c":["","shinagawa-charter-q1",""],"i":false,"f":[[["",{"children":[["slug","shinagawa-charter-q1","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a63443551c7d7d9f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&display=swap","rel":"stylesheet"}],["$","link",null,{"rel":"icon","href":"/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","shinagawa-charter-q1","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8","$undefined",null,["$","$L9",null,{"children":["$La","$Lb",null]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","UNyVARkygs77pKZ7-TMJ8",{"children":[["$","$Lc",null,{"children":"$Ld"}],null]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
11:I[6091,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Header"]
12:I[81068,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Box"]
13:I[17921,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Heading"]
14:I[90310,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Text"]
15:I[7684,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Icon"]
16:I[4618,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"ClientContainer"]
24:I[17264,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Analysis"]
25:I[91548,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Separator"]
27:I[18607,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Footer"]
17:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
18:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
19:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1a:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1b:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
1c:T3f38,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import Any, TypedDict

import numpy as np
import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"
PIPELINE_DIR = ROOT_DIR / "broadlistening" / "pipeline"


def json_serialize_numpy(obj: Any) -> Any:
    """
    Recursively convert NumPy data types to native Python types for JSON serialization.

    Args:
        obj: Any Python object which might contain NumPy data types

    Returns:
        The same object structure with NumPy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: json_serialize_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [json_serialize_numpy(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(json_serialize_numpy(item) for item in obj)
    else:
        return obj


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]
    attributes: dict[str, str] | None
    url: str | None


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config) -> bool:
    try:
        path = f"outputs/{config['output_dir']}/hierarchical_result.json"
        results = {
            "arguments": [],
            "clusters": [],
            "comments": {},
            "propertyMap": {},
            "translations": {},
            "overview": "",
            "config": config,
        }

        arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
        arguments.set_index("arg-id", inplace=True)
        arg_num = len(arguments)
        relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
        comments = pd.read_csv(f"inputs/{config['input']}.csv")
        clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
        labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

        hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

        results["arguments"] = _build_arguments(clusters, comments, relation_df, config)
        results["clusters"] = _build_cluster_value(labels, arg_num)

        # results["comments"] = _build_comments_value(
        #     comments, arguments, hidden_properties_map
        # )
        results["comment_num"] = len(comments)
        results["translations"] = _build_translations(config)
        # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
        results["propertyMap"] = _build_property_map(arguments, comments, hidden_properties_map, config)

        with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
            overview = f.read()
        print("overview")
        print(overview)
        results["overview"] = overview

        # Convert non-serializable NumPy types to native Python types
        results = json_serialize_numpy(results)

        with open(path, "w") as file:
            json.dump(results, file, indent=2, ensure_ascii=False)
        # TODO: サンプリングロジックを実装したいが、現状は全件抽出
        create_custom_intro(config)
        if config["is_pubcom"]:
            add_original_comments(labels, arguments, relation_df, clusters, config)
        return True
    except Exception as e:
        print("error")
        print(e)
        return False


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = PIPELINE_DIR / f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    result_path = PIPELINE_DIR / f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    # LLMプロバイダーとモデル名の判定
    def get_llm_provider_display():
        # configからプロバイダー情報を取得（優先）
        provider = config.get("provider", "openai")
        model = config.get("model", "unknown")

        # プロバイダー名をマッピング
        provider_names = {
            "openai": "OpenAI API",
            "azure": "Azure OpenAI API",
            "openrouter": "OpenRouter API",
            "local": "Local LLM",
        }

        provider_name = provider_names.get(provider, f"{provider} API")
        return f"{provider_name} ({model})"

    llm_provider = get_llm_provider_display()

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対して{llm_provider}を用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(
        intro=intro, processed_num=processed_num, args_count=args_count, llm_provider=llm_provider
    )

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]

    # 基本カラム
    for col in ["x", "y", "source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    # 属性カラムを追加
    attribute_columns = []
    for col in comments.columns:
        # attributeプレフィックスが付いたカラムを探す
        if col.startswith("attribute_"):
            attribute_columns.append(col)
            final_cols.append(col)

    print(f"属性カラム検出: {attribute_columns}")

    # 必要なカラムだけ選択
    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(PIPELINE_DIR / f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(
    clusters: pd.DataFrame, comments: pd.DataFrame, relation_df: pd.DataFrame, config: dict
) -> list[Argument]:
    """
    Build the arguments list including attribute information from original comments

    Args:
        clusters: DataFrame containing cluster information for each argument
        comments: DataFrame containing original comments with attribute columns
        relation_df: DataFrame relating arguments to original comments
        config: Configuration dictionary containing enable_source_link setting
    """
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    # Prepare for merging with original comments to get attributes
    comments_copy = comments.copy()
    comments_copy["comment-id"] = comments_copy["comment-id"].astype(str)

    # Get argument to comment mapping
    arg_comment_map = {}
    if "comment-id" in relation_df.columns:
        relation_df["comment-id"] = relation_df["comment-id"].astype(str)
        arg_comment_map = dict(zip(relation_df["arg-id"], relation_df["comment-id"], strict=False))

    # Find attribute columns in comments dataframe
    attribute_columns = [col for col in comments.columns if col.startswith("attribute_")]
    print(f"属性カラム検出: {attribute_columns}")

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(str(row[cluster_column]))  # Convert to string to ensure serializable

        # Create base argument
        argument: Argument = {
            "arg_id": str(row["arg-id"]),  # Convert to string to ensure serializable
            "argument": str(row["argument"]),
            "x": float(row["x"]),  # Convert to native float
            "y": float(row["y"]),  # Convert to native float
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
            "attributes": None,
            "url": None,
        }

        # Add attributes and URL if available
        if row["arg-id"] in arg_comment_map:
            comment_id = arg_comment_map[row["arg-id"]]
            comment_rows = comments_copy[comments_copy["comment-id"] == comment_id]

            if not comment_rows.empty:
                comment_row = comment_rows.iloc[0]

                # Add URL if available and enabled
                if config.get("enable_source_link", False) and "url" in comment_row and comment_row["url"] is not None:
                    argument["url"] = str(comment_row["url"])

                # Add attributes if available
                if attribute_columns:
                    attributes = {}
                    for attr_col in attribute_columns:
                        # Remove "attribute_" prefix for cleaner attribute names
                        attr_name = attr_col[len("attribute_") :]
                        # Convert potential numpy types to Python native types
                        attr_value = comment_row.get(attr_col, None)
                        if attr_value is not None:
                            if isinstance(attr_value, np.integer):
                                attr_value = int(attr_value)
                            elif isinstance(attr_value, np.floating):
                                attr_value = float(attr_value)
                            elif isinstance(attr_value, np.ndarray):
                                attr_value = attr_value.tolist()
                        attributes[attr_name] = attr_value

                    # Only add non-empty attributes
                    if any(v is not None for v in attributes.values()):
                        argument["attributes"] = attributes

        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=int(total_num),  # Convert to native int
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        # Convert potential NumPy types to native Python types
        level = (
            int(melted_label["level"]) if isinstance(melted_label["level"], int | np.integer) else melted_label["level"]
        )
        cluster_id = str(melted_label["id"])
        label = str(melted_label["label"])
        takeaway = str(melted_label["description"])
        value = (
            int(melted_label["value"]) if isinstance(melted_label["value"], int | np.integer) else melted_label["value"]
        )
        parent = str(melted_label.get("parent", "全体"))

        # Handle density_rank_percentile which might be None or a numeric value
        density_rank = melted_label.get("density_rank_percentile")
        if density_rank is not None:
            if isinstance(density_rank, float | np.floating):
                density_rank = float(density_rank)
            elif isinstance(density_rank, int | np.integer):
                density_rank = int(density_rank)

        cluster_value = Cluster(
            level=level,
            id=cluster_id,
            label=label,
            takeaway=takeaway,
            value=value,
            parent=parent,
            density_rank_percentile=density_rank,
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(PIPELINE_DIR / f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, comments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            value = row[prop] if not pd.isna(row[prop]) else None

            # Convert NumPy types to Python native types
            if value is not None:
                if isinstance(value, np.integer):
                    value = int(value)
                elif isinstance(value, np.floating):
                    value = float(value)
                elif isinstance(value, np.ndarray):
                    value = value.tolist()
                else:
                    # Convert any other types to string to ensure serialization
                    try:
                        value = str(value)
                    except Exception as e:
                        print(f"Error converting value to string: {e}")
                        value = None

            # Make sure arg_id is string
            str_arg_id = str(arg_id)
            property_map[prop][str_arg_id] = value

    return property_map
1d:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
1e:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1f:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
20:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
21:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
22:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
23:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
8:[["$","$L11",null,{}],["$","$L12",null,{"className":"container","mt":"8","children":[["$","$L12",null,{"mx":"auto","maxW":"750px","mb":8,"children":[["$","$L13",null,{"textAlign":"left","fontSize":"xl","mb":5,"children":"レポート"}],["$","$L13",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"日頃から防災に対して心がけていることは何ですか？"}],["$","$L14",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L15",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"154","件"]}],["$","p",null,{"children":"災害時に備えた家庭内備蓄や安全対策の重要性が強調されています。家庭内では備蓄品の管理や賞味期限の確認、地域連携の強化が求められ、避難経路や連絡手段の確立が重要視されています。また、具体的な行動として防災意識の向上や地域住民のコミュニケーション強化が必要であり、家庭内の安全対策や緊急時対応の準備も重要です。"}]]}],["$","$L16",null,{"result":{"arguments":[{"arg_id":"Acsv-6_0","argument":"家庭内での備蓄を行うべき","x":6.189433,"y":8.987854,"p":0,"cluster_ids":["0","1_1","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-6_1","argument":"自宅近くの避難所の確認と避難所までの経路の把握を行うべき","x":3.2474778,"y":7.712496,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-7_0","argument":"被災地で起きたことに関心を持つべきである。","x":2.769211,"y":11.148199,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-7_1","argument":"災害時に直面することをイメージすることで備えにつながる。","x":2.679089,"y":11.437204,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-13_0","argument":"お水と食品と小銭を用意している","x":8.008501,"y":8.624528,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-14_0","argument":"自分ですべて用意するのはむずかしい","x":6.308392,"y":9.297105,"p":0,"cluster_ids":["0","1_1","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-15_0","argument":"防災訓練に参加している","x":1.3105416,"y":10.979687,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-15_1","argument":"携帯トイレを持っている","x":8.017863,"y":9.612693,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-19_0","argument":"1週間分の備蓄が必要である","x":7.48104,"y":9.290264,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-19_1","argument":"町会の訓練に参加している","x":0.98531204,"y":10.838135,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-20_0","argument":"水などを備蓄している。","x":8.376286,"y":8.644717,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-21_0","argument":"水の備蓄をしている。","x":8.582306,"y":8.562422,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-23_0","argument":"備蓄をして、何かあったときに対応できるようにするべきである。","x":7.0950055,"y":9.607,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-23_1","argument":"日頃から地震等を考慮して物を配置したり、火事が起きないように周りの物を掃除するべきである。","x":3.8633184,"y":10.9044895,"p":0,"cluster_ids":["0","1_5","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-24_0","argument":"災害時に家族の集まる場所を決めておくべき","x":2.8137937,"y":8.133365,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-24_1","argument":"3日分の水を確保しておくべき","x":8.293332,"y":8.156729,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-24_2","argument":"地震保険や火災保険に加入すべき","x":3.1483867,"y":11.203066,"p":0,"cluster_ids":["0","1_5","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-27_0","argument":"避難用袋に全部収納している。","x":3.9658298,"y":11.833373,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-31_0","argument":"家族との連絡方法を決めるべきである。","x":2.3953676,"y":8.054167,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-31_1","argument":"家の中を片付けて、上から物が落ちないようにするべきである。","x":4.850157,"y":10.388875,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-31_2","argument":"各部屋に懐中電灯を置いておくべきである。","x":5.1575627,"y":9.987213,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-31_3","argument":"寝室に履物を置くべきである。","x":5.294231,"y":10.28552,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-32_0","argument":"備蓄品を常にストックしているべきである","x":6.851866,"y":9.678589,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-33_0","argument":"消火栓の位置を確認すべき","x":4.27182,"y":7.64711,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-33_1","argument":"消火栓の開栓具の保管場所を確認すべき","x":4.5312204,"y":7.7717333,"p":0,"cluster_ids":["0","1_2","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-33_2","argument":"防災課の人に三輪スクーターの後ろに消火具を積んだ仕様の車輛の導入を検討すべき","x":3.6441092,"y":11.571517,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-33_3","argument":"木造住宅密集地域において三輪スクーターの導入は有効と思われる","x":4.2310786,"y":10.929782,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-34_0","argument":"水などの備蓄が必要である","x":8.484405,"y":8.745064,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-34_1","argument":"防災リュックを作るべきである","x":3.997917,"y":11.793784,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-35_0","argument":"日ごろの防災意識の蓄積が大きく役に立つことがわかった","x":2.2943537,"y":11.778101,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-35_1","argument":"自身と家族の安否確認が大切である","x":2.6039371,"y":7.8295174,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-35_2","argument":"マンション理事会や町会での役割分担よりも日ごろのコミュニケーションが非常に重要である","x":2.1738324,"y":9.039949,"p":0,"cluster_ids":["0","1_2","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-36_0","argument":"毎月の防災訓練は反省ばかりでうまくいったと感じることはあまりない。","x":1.0857716,"y":11.203681,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-36_1","argument":"防災訓練を改善するためには、品川総合福祉センターの職員の防災への意識を高める必要がある。","x":1.18284,"y":11.283021,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-37_0","argument":"防災のヒントになる情報に出会えると、適切な指示や命令を出せるように極力吸収するよう心掛けている。","x":2.4306836,"y":11.742036,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-38_0","argument":"ニュースをきくようにしている","x":1.7748327,"y":10.67396,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-38_1","argument":"町会の防災訓練に参加している","x":1.2423745,"y":10.895431,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-39_1","argument":"品川区の防災訓練に参加している","x":1.3551226,"y":10.883632,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-39_2","argument":"京陽小学校の避難訓練に参加している","x":1.1527729,"y":10.7732,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-40_0","argument":"災害はいつ起こるかわからないため、意識して備える必要があるが、具体的に何をすればよいかわからない。","x":2.7375379,"y":11.484211,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-41_0","argument":"水、食料、トイレ等の意識が高くなった。","x":8.078188,"y":8.16395,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-42_0","argument":"避難場所を把握するべきである。","x":3.3926792,"y":8.04593,"p":0,"cluster_ids":["0","1_2","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-43_0","argument":"水の備蓄を行い、お風呂の水をできるだけためておくべきである。","x":8.554564,"y":8.354399,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-43_1","argument":"缶詰などの期限を把握し、定期的に買い足すべきである。","x":6.949042,"y":8.0199375,"p":0,"cluster_ids":["0","1_1","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-44_0","argument":"出かける前は必ず火の元を確認すべき","x":4.2381673,"y":7.573275,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-45_0","argument":"消火器のある場所を確認している","x":4.4587545,"y":7.82588,"p":0,"cluster_ids":["0","1_2","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-45_1","argument":"防災ブザーの電池切れを確認している","x":4.016928,"y":11.866173,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-46_0","argument":"物品がすぐに使用できるように心掛けるべきである。","x":6.048221,"y":9.952592,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-47_0","argument":"心がけていることはない。","x":6.489296,"y":9.203623,"p":0,"cluster_ids":["0","1_1","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-48_0","argument":"防災グッズを用意するべきである。","x":3.5097406,"y":12.168528,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-49_0","argument":"水が止まる前におふろの水をいっぱいためるべきである。","x":8.7122345,"y":7.988537,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-50_0","argument":"備品の準備が必要である","x":6.188278,"y":10.118833,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-51_0","argument":"備品を置いておくことが重要である","x":5.928277,"y":10.072291,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-52_0","argument":"常にとなり近所と話しをしている。","x":2.4006937,"y":8.716173,"p":0,"cluster_ids":["0","1_2","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-53_0","argument":"非常食を準備している","x":6.96303,"y":8.461914,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-54_0","argument":"1人だけ逃げるのはこわい","x":3.2253618,"y":8.175852,"p":0,"cluster_ids":["0","1_2","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-54_1","argument":"友人や隣の知っている人と一緒に行けば安心できる","x":2.6182952,"y":8.476536,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-55_0","argument":"家までの徒歩のルートを確認しておくべき","x":3.3853078,"y":7.5790854,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-55_1","argument":"家族間での連絡手段を決めておくべき","x":2.34053,"y":8.21103,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-56_0","argument":"食品の備蓄を行うべき","x":7.50943,"y":8.863344,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-56_1","argument":"子供たちに親の連絡先を覚えさせるべき","x":2.4613543,"y":8.0748415,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-57_0","argument":"避難地区の確保と確認が必要である","x":3.4650137,"y":7.9504776,"p":0,"cluster_ids":["0","1_2","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-57_1","argument":"常備食品と飲料の準備が必要である","x":7.534306,"y":8.509281,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-58_0","argument":"防災グッズを備えておくべき","x":3.4598129,"y":12.086047,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-58_1","argument":"飲料水を備蓄すべき","x":8.12406,"y":8.505651,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-58_2","argument":"避難場所や経路を家族内で共有すべき","x":2.9046323,"y":8.025349,"p":0,"cluster_ids":["0","1_2","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-59_0","argument":"避難場所を意識することが重要である","x":3.4326744,"y":8.30063,"p":0,"cluster_ids":["0","1_2","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-60_0","argument":"品川区が開催するイベントで電気の広げ方を学んだ","x":1.5365148,"y":10.576565,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-60_1","argument":"日テレのそらじろーが行う防災イベントで熱中症対策を学んだ","x":1.455746,"y":10.875957,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-61_0","argument":"品川区の対応は他県に住んでいる人々にとってうらやましい","x":2.2175362,"y":9.25746,"p":0,"cluster_ids":["0","1_2","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-61_1","argument":"品川区の対応はとても大事なことなので、今後もよろしくお願いします","x":2.2374213,"y":9.489423,"p":0,"cluster_ids":["0","1_2","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-62_0","argument":"防災セットを常備すべきである","x":3.1220307,"y":12.227618,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-63_0","argument":"水の保存は重要である。","x":8.273498,"y":7.7803526,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-64_0","argument":"防災グッズの準備は重要である。","x":3.3901453,"y":12.5819,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-65_0","argument":"防災リュックを活用している","x":4.0561094,"y":11.964802,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-65_1","argument":"常備食料品を普段使いして期限切れにならないように意識している","x":7.0667048,"y":7.80285,"p":0,"cluster_ids":["0","1_1","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-66_0","argument":"非常用の道具を家族分用意すべきである。","x":5.2606945,"y":9.489955,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-67_0","argument":"水道が止まることを想定して、ふろ水を捨てず夜までためておき、トイレなどに使うことを考えている。","x":8.681413,"y":8.053222,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-68_0","argument":"防災バッグを用意すべき","x":3.6534004,"y":11.993263,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-68_1","argument":"水の備蓄を行うべき","x":8.354406,"y":8.500371,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-68_2","argument":"家具のすべりどめを施すべき","x":4.737603,"y":10.518208,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-69_0","argument":"いざという時に備えてリュックを準備すべきである。","x":5.5058336,"y":11.114595,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-70_0","argument":"避難経路は安全確保のために重要である。","x":3.312568,"y":7.823327,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-71_0","argument":"家具の転倒対策を講じるべきである。","x":4.6375413,"y":10.589588,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-72_0","argument":"防災グッズをそろえるべきである。","x":3.4862,"y":12.327912,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-72_1","argument":"防災グッズの中身は定期的に確認し、期限が切れているものやサイズアウトした服を見直す必要がある。","x":3.737013,"y":12.544017,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-73_0","argument":"水を常備しているべきである","x":8.432595,"y":7.94262,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-74_0","argument":"防災グッズの見直しが必要である。","x":3.5640113,"y":12.381734,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-76_0","argument":"ローリングストックは非常時に備えるための重要な手段である。","x":6.65529,"y":10.060676,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-76_1","argument":"防災トイレは災害時の衛生管理において必要不可欠である。","x":3.394058,"y":12.897646,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-76_2","argument":"防災バッグは緊急時に必要な物資をまとめておくために重要である。","x":3.5132945,"y":12.602055,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-77_0","argument":"食品の保管を意識している","x":7.65671,"y":7.7727094,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-77_1","argument":"水の保管を意識している","x":8.24231,"y":7.714703,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-78_0","argument":"備蓄品をそなえている","x":6.8568645,"y":9.5106535,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-78_1","argument":"水を10Lくらい備蓄している","x":8.68227,"y":8.469485,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-79_0","argument":"コンロの火の確認が必要である。","x":4.4148593,"y":7.653919,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-80_0","argument":"水分、食料のローリングストックを行うべきである。","x":7.914947,"y":7.5615253,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-81_0","argument":"地震対策として棚をしめるべきである。","x":3.7840729,"y":10.973794,"p":0,"cluster_ids":["0","1_5","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-81_1","argument":"集合場所を認識することが重要である。","x":3.3720436,"y":8.317877,"p":0,"cluster_ids":["0","1_2","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-83_0","argument":"食料を備蓄することはうまくいった","x":7.5416417,"y":8.849022,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-83_1","argument":"家具の固定などの改善が必要","x":4.745097,"y":10.555346,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-84_0","argument":"備蓄食品を用意するべきである。","x":7.125231,"y":9.118643,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-85_0","argument":"水、食品を多めに保管すべき","x":7.8370576,"y":7.590866,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-85_1","argument":"日常の薬も少し多めに持っておくべき","x":6.897359,"y":7.805441,"p":0,"cluster_ids":["0","1_1","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-85_2","argument":"リュックを準備すべき","x":5.7314367,"y":10.921024,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-86_0","argument":"普段から身のまわりをよく見ておくべき","x":7.0341067,"y":7.468137,"p":0,"cluster_ids":["0","1_1","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-86_1","argument":"訓練に参加すべき","x":0.8921044,"y":10.854968,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-87_0","argument":"防災訓練に参加するべきである。","x":1.4904331,"y":11.314478,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-89_0","argument":"家具の転倒防止対策が必要である。","x":4.525239,"y":10.676795,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-90_0","argument":"避難場所を確認する必要がある。","x":3.807156,"y":7.947568,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-91_0","argument":"エリア毎の被災リスクを意識すべき","x":2.80199,"y":10.981347,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-92_0","argument":"防災用バックを用意するべきである","x":3.8022716,"y":11.86318,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-92_1","argument":"非常食の準備をするべきである","x":6.9516044,"y":8.602103,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-93_0","argument":"缶詰等を買っているが、賞味期限切れでうまく備えきれていない。","x":7.332753,"y":8.127323,"p":0,"cluster_ids":["0","1_1","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-94_0","argument":"防災グッズを準備するべきである。","x":3.8029003,"y":12.434768,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-95_0","argument":"ウォーターサーバーは停電時にも使用できるため設置すべきである。","x":8.804326,"y":8.603152,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-96_0","argument":"防災セットを常備するべきである。","x":3.1649406,"y":12.051553,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-97_0","argument":"防災士の資格取得のために勉強中である","x":1.3874719,"y":11.186868,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-97_1","argument":"教室講座に防災を取り入れた","x":1.5651734,"y":11.078665,"p":0,"cluster_ids":["0","1_4","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-98_0","argument":"AEDの場所を確認すべき","x":4.2335205,"y":7.758844,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-98_1","argument":"非常口の場所を確認すべき","x":4.053754,"y":7.655262,"p":0,"cluster_ids":["0","1_2","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-98_2","argument":"家の貯蓄の数を把握すべき","x":6.606171,"y":8.9618435,"p":0,"cluster_ids":["0","1_1","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-99_0","argument":"いつ災害が起こっても生活維持が図れるよう備蓄品を用意しておくべきである。","x":2.852798,"y":11.658097,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-100_0","argument":"防災訓練に参加することで知識だけでなく、地域の人とコミュニケーションをとることが大事である。","x":1.0842211,"y":11.313206,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-101_0","argument":"米をたくわえている","x":7.1844873,"y":8.651365,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-102_0","argument":"防災品の準備は重要である。","x":3.104278,"y":12.462123,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-103_0","argument":"体験などにはなるべく参加し、知識をつけておくべきである。","x":0.75170237,"y":11.17996,"p":0,"cluster_ids":["0","1_4","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-104_0","argument":"備蓄品のストックは重要である。","x":6.937541,"y":9.76824,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-105_0","argument":"ハザードマップや避難場所を確認すべき","x":3.687126,"y":7.7749276,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-105_1","argument":"備蓄品の賞味期限を確認すべき","x":7.0297008,"y":9.055016,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-106_0","argument":"災害備品を持つべきである","x":3.1570375,"y":11.76894,"p":0,"cluster_ids":["0","1_5","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-107_0","argument":"備蓄は重要である。","x":7.300249,"y":9.437322,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-108_0","argument":"避難経路を把握すべき","x":3.1743789,"y":7.7919426,"p":0,"cluster_ids":["0","1_2","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-108_1","argument":"避難グッズを定期的に見直すべき","x":4.007752,"y":12.4828615,"p":0,"cluster_ids":["0","1_5","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-111_0","argument":"防災リュックを用意すべきである。","x":3.9076738,"y":12.121929,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-112_0","argument":"食品や水などの備蓄が必要である","x":7.9429903,"y":8.872914,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-114_0","argument":"備蓄は重要である","x":7.121922,"y":9.521231,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-114_1","argument":"家具の転倒防止対策が必要である","x":4.6155157,"y":10.604617,"p":0,"cluster_ids":["0","1_3","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-115_0","argument":"アルファ米の新しい食べ方を知りたい。","x":7.1179914,"y":8.611288,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-116_0","argument":"ローリングストックは、災害時の備蓄や食品ロスの削減に有効な手法である。","x":6.6051126,"y":9.998062,"p":0,"cluster_ids":["0","1_1","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-117_0","argument":"まわりの人と協力するべきである。","x":2.4041443,"y":8.686944,"p":0,"cluster_ids":["0","1_2","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-118_0","argument":"水と食料を7日分備蓄するべきである。","x":8.027156,"y":8.109344,"p":0,"cluster_ids":["0","1_1","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-119_0","argument":"備蓄セットは更新すべきである。","x":7.192349,"y":9.671298,"p":0,"cluster_ids":["0","1_1","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-119_1","argument":"水のタンクの入れ替えが必要である。","x":8.828421,"y":8.331156,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-120_0","argument":"防災イベントに参加すべき","x":1.9651195,"y":11.382357,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-120_1","argument":"家の立地を考慮して備蓄は特に必要ない","x":6.5298758,"y":9.130467,"p":0,"cluster_ids":["0","1_1","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-121_0","argument":"備蓄品（食料）は期限がくると取りかえるべきである","x":7.3010206,"y":9.160611,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-121_1","argument":"備蓄（トイレ）は期限がくると取りかえるべきである","x":7.642889,"y":9.60042,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-122_0","argument":"東日本大震災から学んでガソリン20Lと灯油40Lを確保している。","x":3.027138,"y":11.6322365,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-123_0","argument":"消火器には耐用年数があるため、定期的な点検と交換が必要である。","x":4.7583103,"y":7.9248137,"p":0,"cluster_ids":["0","1_2","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-125_0","argument":"水、備蓄（食料品）、簡易トイレは重要である","x":8.263417,"y":9.141895,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-125_1","argument":"トイレは特に重要である","x":8.219703,"y":9.518201,"p":0,"cluster_ids":["0","1_1","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-126_0","argument":"地震対策を強化すべき","x":3.4563706,"y":11.052432,"p":0,"cluster_ids":["0","1_5","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-126_1","argument":"防災食の重要性を認識し、備蓄を進めるべき","x":2.5699353,"y":12.062465,"p":0,"cluster_ids":["0","1_5","2_2"],"attributes":null,"url":null}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":154,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_1","label":"災害時に備えた家庭内備蓄の実践と管理","takeaway":"家庭内での備蓄は、災害時に必要な水や食料を確保するための重要な対策です。特に、備蓄品の管理や賞味期限の確認、ローリングストックの実践が求められています。また、日常生活における備蓄意識の向上や、非常時に迅速に対応できるようにするための準備が強調されており、家庭内での備蓄の重要性が再認識されています。","value":59,"parent":"0","density_rank_percentile":1},{"level":1,"id":"1_2","label":"災害時の安全確保と地域連携の強化","takeaway":"災害時における安全確保のためには、避難経路や避難場所の事前確認が不可欠です。また、家族間での連絡手段の確立や安否確認の重要性が強調されており、特に子供たちに親の連絡先を覚えさせることが求められています。さらに、消火器や消火栓の管理、地域住民同士のコミュニケーションや協力が、危機的状況における迅速な対応を可能にし、地域全体の安全意識を高めることが期待されています。","value":31,"parent":"0","density_rank_percentile":0.8},{"level":1,"id":"1_5","label":"災害に備えるための具体的な行動と意識の向上","takeaway":"災害時に備えるための具体的な行動や意識の重要性が強調されています。ガソリンや灯油の確保、防災食の備蓄、地震や火災に対する具体的な対策、さらには防災イベントへの参加を通じて、日常的に防災意識を高めることが求められています。また、防災グッズの定期的な見直しや、避難用具の準備、物の配置や掃除といった具体的な行動が提案され、個人や家庭が災害に対する備えを強化することの重要性が訴えられています。","value":36,"parent":"0","density_rank_percentile":0.6},{"level":1,"id":"1_4","label":"地域住民の防災意識向上とコミュニケーション強化の取り組み","takeaway":"地域の学校や町会が主催する防災訓練やイベントへの参加を通じて、住民が防災に関する知識やスキルを身につけることが重要視されています。特に、京陽小学校や品川区の活動に参加することで、地域住民同士のコミュニケーションが促進され、相互の防災意識が高まることが期待されています。また、防災士の資格取得や訓練の改善に向けた具体的な取り組みも求められています。","value":16,"parent":"0","density_rank_percentile":0.2},{"level":1,"id":"1_3","label":"家庭内安全対策の強化と緊急時対応の準備","takeaway":"家庭内での安全性を高めるための具体的な対策が求められています。特に、家具の転倒防止策や固定、懐中電灯や非常用道具の準備、緊急時に備えたリュックの用意などが重要視されています。これにより、家族の安全を確保し、緊急時に迅速に対応できる体制を整えることが期待されています。","value":12,"parent":"0","density_rank_percentile":0.4},{"level":2,"id":"2_13","label":"家庭内備蓄の重要性と実践の難しさ","takeaway":"この意見グループは、家庭内での備蓄の必要性についての認識と、実際に備蓄を行うことの難しさに焦点を当てています。特に、備蓄を行うべきという意見がある一方で、実際に自分で全てを用意することが難しいという現実や、家の立地によって備蓄の必要性が変わるという視点が含まれています。","value":5,"parent":"1_1","density_rank_percentile":0.36},{"level":2,"id":"2_0","label":"避難経路と安全確認の重要性","takeaway":"この意見グループは、災害時における安全な避難を確保するために、ハザードマップや避難場所の確認、そして自宅から避難所までの徒歩ルートの把握が重要であるという点に焦点を当てています。特に、避難経路の事前確認が安全確保に寄与するという認識が強調されています。","value":5,"parent":"1_2","density_rank_percentile":0.32},{"level":2,"id":"2_2","label":"災害対策と備蓄の重要性","takeaway":"この意見グループは、災害時に備えるための具体的な行動や意識の重要性に焦点を当てています。災害がいつ起こるかわからないため、ガソリンや灯油の確保、防災食の備蓄、地域の被災リスクの認識、さらには防災イベントへの参加を通じて、日常的に防災意識を高めることが求められています。","value":10,"parent":"1_5","density_rank_percentile":0.96},{"level":2,"id":"2_3","label":"災害時に備えた水の備蓄と利用法","takeaway":"この意見グループは、災害や水道の停止に備えて水を備蓄する重要性に焦点を当てています。具体的には、風呂の水をトイレなどに利用するためにためておくことや、ウォーターサーバーの設置、さらには水のタンクの入れ替えなど、実際的な備蓄方法や利用法についての意見が集まっています。","value":12,"parent":"1_1","density_rank_percentile":0.84},{"level":2,"id":"2_20","label":"地域に根ざした防災教育と訓練の推進","takeaway":"この意見グループは、地域の学校や町会、区が主催する防災訓練やイベントに参加することを通じて、防災意識を高める取り組みが中心です。特に、京陽小学校や品川区の活動に参加することで、地域住民が防災に対する知識やスキルを身につける重要性が強調されています。","value":8,"parent":"1_4","density_rank_percentile":0.48},{"level":2,"id":"2_1","label":"災害時における生活必需品の備蓄と管理","takeaway":"この意見グループは、災害時に必要な生活必需品としての食品や水、トイレの備蓄の重要性を強調しています。また、備蓄品の期限管理や携帯トイレの所持についても言及されており、災害に備えるための具体的な対策が求められています。","value":6,"parent":"1_1","density_rank_percentile":1},{"level":2,"id":"2_6","label":"防災訓練の重要性と地域コミュニケーションの促進","takeaway":"この意見グループは、防災訓練への参加の重要性や、知識の習得、地域住民とのコミュニケーションの強化に焦点を当てています。また、防災士の資格取得や訓練の改善に関する意見も含まれており、地域の防災意識を高めるための具体的な取り組みが求められています。","value":8,"parent":"1_4","density_rank_percentile":0.68},{"level":2,"id":"2_24","label":"備蓄の重要性と継続的な管理","takeaway":"この意見グループは、備蓄の必要性とその重要性に焦点を当てています。特に、何かあったときに迅速に対応できるようにするための備蓄品のストックや更新の重要性が強調されており、常に備蓄を維持することが求められています。","value":7,"parent":"1_1","density_rank_percentile":0.24},{"level":2,"id":"2_14","label":"地震・火災対策の重要性と実践","takeaway":"この意見グループは、地震や火災に対する備えの重要性を強調しており、具体的には物の配置や掃除、保険への加入、さらなる対策の強化が必要であるという意見が中心です。安全な生活環境を維持するための具体的な行動が提案されています。","value":4,"parent":"1_5","density_rank_percentile":0.72},{"level":2,"id":"2_12","label":"家族の安全確保と連絡体制の重要性","takeaway":"この意見グループは、災害時における家族の安全を確保するための具体的な対策に焦点を当てています。避難場所や経路の共有、連絡手段の確立、安否確認の重要性など、家族間でのコミュニケーションと連携を強化することが求められています。","value":7,"parent":"1_2","density_rank_percentile":0.64},{"level":2,"id":"2_9","label":"災害対策としての水分・食料の備蓄意識","takeaway":"この意見グループは、災害時に備えた水分や食料のローリングストックや備蓄の重要性に対する認識が高まっていることを示しています。特に、水の保存や常備、3日分や7日分の備蓄を意識することで、緊急時に必要な資源を確保することが強調されています。","value":9,"parent":"1_1","density_rank_percentile":0.88},{"level":2,"id":"2_15","label":"防災対策の重要性と具体的な準備","takeaway":"この意見グループは、防災に対する意識の高まりと具体的な準備の必要性を強調しています。防災バッグやリュックの用意、避難用具の収納、さらには消火具を搭載した車両の導入提案など、実際の防災対策に関する具体的なアイデアや行動が中心となっています。","value":8,"parent":"1_5","density_rank_percentile":0.4},{"level":2,"id":"2_4","label":"家具の転倒防止と安全対策の重要性","takeaway":"この意見グループは、家具の転倒防止や固定に関する必要性を強調しており、特に木造住宅密集地域における安全対策の重要性が示されています。家具のすべりどめや転倒防止策を講じることで、家庭内の安全性を高めることが求められています。","value":7,"parent":"1_3","density_rank_percentile":0.28},{"level":2,"id":"2_23","label":"家庭内の安全対策と利便性向上","takeaway":"この意見グループは、家庭内での安全性を高めるための具体的な対策に焦点を当てています。懐中電灯や非常用の道具を用意することで、緊急時の対応力を向上させ、また寝室に履物を置くことで利便性を高めることが提案されています。これらは、家族の安全と快適さを確保するための重要な要素です。","value":3,"parent":"1_3","density_rank_percentile":0.76},{"level":2,"id":"2_16","label":"安全確認と避難対策の重要性","takeaway":"この意見グループは、緊急時に備えた安全確認の必要性に焦点を当てています。AEDや避難場所、消火栓、火の元の確認など、災害や火災に対する事前の準備が重要であるという意見が集まっています。これにより、危機的状況における迅速な対応が可能になることが強調されています。","value":6,"parent":"1_2","density_rank_percentile":0.44},{"level":2,"id":"2_11","label":"消火器と消火栓の管理と点検の重要性","takeaway":"この意見グループは、消火器や消火栓に関する管理の重要性を強調しています。耐用年数がある消火器の定期的な点検や交換の必要性、消火器の設置場所の確認、さらには消火栓の開栓具の保管場所の確認が必要であるという点が共通しており、火災対策における安全管理の徹底を求める意見が中心です。","value":3,"parent":"1_2","density_rank_percentile":0.12},{"level":2,"id":"2_7","label":"地域コミュニティの重要性と協力の促進","takeaway":"この意見グループは、地域住民同士のコミュニケーションや協力の重要性に焦点を当てています。品川区の対応に対する評価や、日常的なコミュニケーションの必要性が強調されており、地域社会のつながりを深めることが重要であるという意見が中心です。","value":5,"parent":"1_2","density_rank_percentile":0.8},{"level":2,"id":"2_17","label":"避難場所の重要性と集合意識の強化","takeaway":"この意見グループは、避難場所の把握や集合場所の認識が重要であるという共通の認識を持っています。特に、個人の安全だけでなく、集団での行動の重要性が強調されており、避難時の不安を軽減するために、事前に避難場所を確認し、意識することが求められています。","value":5,"parent":"1_2","density_rank_percentile":0.2},{"level":2,"id":"2_5","label":"日常生活における備蓄管理と意識向上","takeaway":"この意見グループは、日常生活における食料品や薬の備蓄管理の重要性に焦点を当てています。特に、缶詰や常備食料品の賞味期限を把握し、定期的に補充することや、普段から意識して使用することで、無駄を減らし、備蓄の有効活用を図ることが求められています。","value":5,"parent":"1_1","density_rank_percentile":0.52},{"level":2,"id":"2_10","label":"備品管理と即時使用の重要性","takeaway":"この意見グループは、業務における備品の準備と管理の重要性に焦点を当てています。備品を適切に準備し、常に使用可能な状態に保つことが、業務の効率化やスムーズな運営に寄与するという考えが中心です。","value":3,"parent":"1_1","density_rank_percentile":0.08},{"level":2,"id":"2_8","label":"防災意識の向上と備えの重要性","takeaway":"この意見グループは、災害に備えるための防災セットや防災グッズの常備が重要であるという共通の認識を示しています。各意見は、災害時に備えることの必要性や重要性を強調しており、個人や家庭が防災対策を講じることの重要性を訴えています。","value":7,"parent":"1_5","density_rank_percentile":0.6},{"level":2,"id":"2_22","label":"非常食と備蓄食品の重要性と管理","takeaway":"この意見グループは、非常食や備蓄食品の準備とその管理に関する重要性を強調しています。特に、賞味期限の確認や新しい食べ方の探求、備蓄の必要性についての意見が多く、災害時や緊急時に備えるための具体的な行動が求められています。","value":10,"parent":"1_1","density_rank_percentile":0.92},{"level":2,"id":"2_19","label":"防災グッズの重要性と定期的な見直しの必要性","takeaway":"この意見グループは、災害時における衛生管理や緊急時の物資準備に関連する防災グッズの重要性を強調しています。また、これらのグッズは定期的に見直し、内容物の確認や更新が必要であるという意見が中心となっており、実際の備えの質を向上させるための具体的な行動が求められています。","value":7,"parent":"1_5","density_rank_percentile":0.56},{"level":2,"id":"2_18","label":"緊急時に備えたリュックの準備","takeaway":"この意見グループは、緊急事態に備えるためにリュックを準備する重要性を強調しています。特に、予期しない状況に対して迅速に対応できるよう、必要な物品を整理して持ち運ぶためのリュックの必要性が中心となっています。","value":2,"parent":"1_3","density_rank_percentile":0.16},{"level":2,"id":"2_21","label":"非常時備蓄と食品ロス削減のためのローリングストック","takeaway":"この意見グループは、ローリングストックが非常時に備えるための重要な手段であり、災害時の備蓄や食品ロスの削減に寄与することに焦点を当てています。特に、効率的な在庫管理と持続可能な消費を促進する方法としてのローリングストックの有用性が強調されています。","value":2,"parent":"1_1","density_rank_percentile":0.04}],"comments":{},"propertyMap":{},"translations":{},"overview":"災害時に備えた家庭内備蓄や安全対策の重要性が強調されています。家庭内では備蓄品の管理や賞味期限の確認、地域連携の強化が求められ、避難経路や連絡手段の確立が重要視されています。また、具体的な行動として防災意識の向上や地域住民のコミュニケーション強化が必要であり、家庭内の安全対策や緊急時対応の準備も重要です。","config":{"name":"shinagawa-charter-q1","input":"shinagawa-charter-q1","question":"日頃から防災に対して心がけていることは何ですか？","intro":"しながわ防災区民憲章に対する意見募集の結果です。\n分析対象となったデータの件数は126件で、これらのデータに対してOpenAI API (gpt-4o-mini)を用いて154件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","provider":"openai","is_pubcom":true,"is_embedded_at_local":false,"local_llm_address":null,"extraction":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":126,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$17","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[5,25],"source_code":"$18"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1a","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1c"},"enable_source_link":false,"output_dir":"shinagawa-charter-q1","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"$1d"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2025-11-14T01:03:42.502787","completed_jobs":[{"step":"extraction","completed":"2025-11-14T01:05:09.451103","duration":86.941461,"params":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":126,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$1e","model":"gpt-4o-mini"},"token_usage":58877},{"step":"embedding","completed":"2025-11-14T01:05:13.195993","duration":3.742468,"params":{"model":"text-embedding-3-small","source_code":"$1f"},"token_usage":0},{"step":"hierarchical_clustering","completed":"2025-11-14T01:05:20.258693","duration":7.060614,"params":{"cluster_nums":[5,25],"source_code":"$20"},"token_usage":0},{"step":"hierarchical_initial_labelling","completed":"2025-11-14T01:05:32.601265","duration":12.339368,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$21","model":"gpt-4o-mini"},"token_usage":18792},{"step":"hierarchical_merge_labelling","completed":"2025-11-14T01:05:39.905645","duration":7.30176,"params":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$22","model":"gpt-4o-mini"},"token_usage":9555},{"step":"hierarchical_overview","completed":"2025-11-14T01:06:03.729257","duration":23.820839,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$23","model":"gpt-4o-mini"},"token_usage":1169}],"total_token_usage":88393,"token_usage_input":80899,"token_usage_output":7494,"lock_until":"2025-11-14T01:11:03.731697","current_job":"hierarchical_aggregation","current_job_started":"2025-11-14T01:06:03.731691","estimated_cost":0.01663125,"current_job_progress":null,"current_jop_tasks":null},"comment_num":126,"visibility":"public"}}],["$","$L24",null,{"result":"$8:1:props:children:1:props:result"}],["$","$L12",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}],["$","$L25",null,{"my":12,"maxW":"750px","mx":"auto"}],["$","$L12",null,{"maxW":"750px","mx":"auto","mb":24,"children":"$L26"}]]}],["$","$L27",null,{"meta":{"reporter":"一般社団法人コード・フォー・ジャパン","message":"ともに考え、ともにつくる社会をビジョンに、Decidimの日本での活用などデジタル民主主義を推進しています。","webLink":"https://www.code4japan.org/","privacyLink":"https://www.code4japan.org/privacy-policy","termsLink":null,"brandColor":"#2577B1","isDefault":false}}]]
b:null
f:[["$","title","0",{"children":"日頃から防災に対して心がけていることは何ですか？ - 一般社団法人コード・フォー・ジャパン"}],["$","meta","1",{"name":"description","content":"災害時に備えた家庭内備蓄や安全対策の重要性が強調されています。家庭内では備蓄品の管理や賞味期限の確認、地域連携の強化が求められ、避難経路や連絡手段の確立が重要視されています。また、具体的な行動として防災意識の向上や地域住民のコミュニケーション強化が必要であり、家庭内の安全対策や緊急時対応の準備も重要です。"}],["$","meta","2",{"property":"og:title","content":"日頃から防災に対して心がけていることは何ですか？ - 一般社団法人コード・フォー・ジャパン"}],["$","meta","3",{"property":"og:description","content":"災害時に備えた家庭内備蓄や安全対策の重要性が強調されています。家庭内では備蓄品の管理や賞味期限の確認、地域連携の強化が求められ、避難経路や連絡手段の確立が重要視されています。また、具体的な行動として防災意識の向上や地域住民のコミュニケーション強化が必要であり、家庭内の安全対策や緊急時対応の準備も重要です。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/shinagawa-charter-q1/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"日頃から防災に対して心がけていることは何ですか？ - 一般社団法人コード・フォー・ジャパン"}],["$","meta","7",{"name":"twitter:description","content":"災害時に備えた家庭内備蓄や安全対策の重要性が強調されています。家庭内では備蓄品の管理や賞味期限の確認、地域連携の強化が求められ、避難経路や連絡手段の確立が重要視されています。また、具体的な行動として防災意識の向上や地域住民のコミュニケーション強化が必要であり、家庭内の安全対策や緊急時対応の準備も重要です。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/shinagawa-charter-q1/opengraph-image.png"}]]
28:I[81499,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"ReporterContent"]
26:["$","$L28",null,{"meta":"$8:2:props:meta","children":"$L29"}]
2a:I[89248,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Image"]
29:["$","$L2a",null,{"src":"/meta/reporter.png","alt":"一般社団法人コード・フォー・ジャパン","maxW":"150px"}]
