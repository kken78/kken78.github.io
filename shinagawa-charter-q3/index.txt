1:"$Sreact.fragment"
2:I[8708,["315","static/chunks/315-19ca0205aa2fdd55.js","458","static/chunks/458-4db2abcd82f4c9c6.js","124","static/chunks/124-8b6d8ff5759d10e8.js","48","static/chunks/48-3a76f8478d544215.js","177","static/chunks/app/layout-4ee75bbda26c9e8d.js"],"Provider"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[32176,["315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","335","static/chunks/app/%5Bslug%5D/error-b85ff2b7d7b78a5a.js"],"default"]
6:I[6874,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],""]
7:I[38567,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Button"]
9:I[59665,[],"OutletBoundary"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/_next/static/css/a63443551c7d7d9f.css","style"]
0:{"P":null,"b":"A7FQK1gn7KAabvCUjbGdL","p":"","c":["","shinagawa-charter-q3",""],"i":false,"f":[[["",{"children":[["slug","shinagawa-charter-q3","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a63443551c7d7d9f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&display=swap","rel":"stylesheet"}],["$","link",null,{"rel":"icon","href":"/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","shinagawa-charter-q3","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8","$undefined",null,["$","$L9",null,{"children":["$La","$Lb",null]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","uV4mbSeGpIg7YAGZt6gE5",{"children":[["$","$Lc",null,{"children":"$Ld"}],null]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
11:I[6091,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Header"]
12:I[81068,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Box"]
13:I[17921,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Heading"]
14:I[90310,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Text"]
15:I[7684,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Icon"]
16:I[4618,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"ClientContainer"]
24:I[17264,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Analysis"]
25:I[91548,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Separator"]
27:I[18607,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Footer"]
17:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
18:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
19:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1a:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1b:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
1c:T3f38,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import Any, TypedDict

import numpy as np
import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"
PIPELINE_DIR = ROOT_DIR / "broadlistening" / "pipeline"


def json_serialize_numpy(obj: Any) -> Any:
    """
    Recursively convert NumPy data types to native Python types for JSON serialization.

    Args:
        obj: Any Python object which might contain NumPy data types

    Returns:
        The same object structure with NumPy types converted to Python native types
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: json_serialize_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [json_serialize_numpy(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(json_serialize_numpy(item) for item in obj)
    else:
        return obj


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]
    attributes: dict[str, str] | None
    url: str | None


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config) -> bool:
    try:
        path = f"outputs/{config['output_dir']}/hierarchical_result.json"
        results = {
            "arguments": [],
            "clusters": [],
            "comments": {},
            "propertyMap": {},
            "translations": {},
            "overview": "",
            "config": config,
        }

        arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
        arguments.set_index("arg-id", inplace=True)
        arg_num = len(arguments)
        relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
        comments = pd.read_csv(f"inputs/{config['input']}.csv")
        clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
        labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

        hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

        results["arguments"] = _build_arguments(clusters, comments, relation_df, config)
        results["clusters"] = _build_cluster_value(labels, arg_num)

        # results["comments"] = _build_comments_value(
        #     comments, arguments, hidden_properties_map
        # )
        results["comment_num"] = len(comments)
        results["translations"] = _build_translations(config)
        # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
        results["propertyMap"] = _build_property_map(arguments, comments, hidden_properties_map, config)

        with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
            overview = f.read()
        print("overview")
        print(overview)
        results["overview"] = overview

        # Convert non-serializable NumPy types to native Python types
        results = json_serialize_numpy(results)

        with open(path, "w") as file:
            json.dump(results, file, indent=2, ensure_ascii=False)
        # TODO: サンプリングロジックを実装したいが、現状は全件抽出
        create_custom_intro(config)
        if config["is_pubcom"]:
            add_original_comments(labels, arguments, relation_df, clusters, config)
        return True
    except Exception as e:
        print("error")
        print(e)
        return False


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = PIPELINE_DIR / f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    result_path = PIPELINE_DIR / f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    # LLMプロバイダーとモデル名の判定
    def get_llm_provider_display():
        # configからプロバイダー情報を取得（優先）
        provider = config.get("provider", "openai")
        model = config.get("model", "unknown")

        # プロバイダー名をマッピング
        provider_names = {
            "openai": "OpenAI API",
            "azure": "Azure OpenAI API",
            "openrouter": "OpenRouter API",
            "local": "Local LLM",
        }

        provider_name = provider_names.get(provider, f"{provider} API")
        return f"{provider_name} ({model})"

    llm_provider = get_llm_provider_display()

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対して{llm_provider}を用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(
        intro=intro, processed_num=processed_num, args_count=args_count, llm_provider=llm_provider
    )

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(PIPELINE_DIR / f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]

    # 基本カラム
    for col in ["x", "y", "source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    # 属性カラムを追加
    attribute_columns = []
    for col in comments.columns:
        # attributeプレフィックスが付いたカラムを探す
        if col.startswith("attribute_"):
            attribute_columns.append(col)
            final_cols.append(col)

    print(f"属性カラム検出: {attribute_columns}")

    # 必要なカラムだけ選択
    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(PIPELINE_DIR / f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(
    clusters: pd.DataFrame, comments: pd.DataFrame, relation_df: pd.DataFrame, config: dict
) -> list[Argument]:
    """
    Build the arguments list including attribute information from original comments

    Args:
        clusters: DataFrame containing cluster information for each argument
        comments: DataFrame containing original comments with attribute columns
        relation_df: DataFrame relating arguments to original comments
        config: Configuration dictionary containing enable_source_link setting
    """
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    # Prepare for merging with original comments to get attributes
    comments_copy = comments.copy()
    comments_copy["comment-id"] = comments_copy["comment-id"].astype(str)

    # Get argument to comment mapping
    arg_comment_map = {}
    if "comment-id" in relation_df.columns:
        relation_df["comment-id"] = relation_df["comment-id"].astype(str)
        arg_comment_map = dict(zip(relation_df["arg-id"], relation_df["comment-id"], strict=False))

    # Find attribute columns in comments dataframe
    attribute_columns = [col for col in comments.columns if col.startswith("attribute_")]
    print(f"属性カラム検出: {attribute_columns}")

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(str(row[cluster_column]))  # Convert to string to ensure serializable

        # Create base argument
        argument: Argument = {
            "arg_id": str(row["arg-id"]),  # Convert to string to ensure serializable
            "argument": str(row["argument"]),
            "x": float(row["x"]),  # Convert to native float
            "y": float(row["y"]),  # Convert to native float
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
            "attributes": None,
            "url": None,
        }

        # Add attributes and URL if available
        if row["arg-id"] in arg_comment_map:
            comment_id = arg_comment_map[row["arg-id"]]
            comment_rows = comments_copy[comments_copy["comment-id"] == comment_id]

            if not comment_rows.empty:
                comment_row = comment_rows.iloc[0]

                # Add URL if available and enabled
                if config.get("enable_source_link", False) and "url" in comment_row and comment_row["url"] is not None:
                    argument["url"] = str(comment_row["url"])

                # Add attributes if available
                if attribute_columns:
                    attributes = {}
                    for attr_col in attribute_columns:
                        # Remove "attribute_" prefix for cleaner attribute names
                        attr_name = attr_col[len("attribute_") :]
                        # Convert potential numpy types to Python native types
                        attr_value = comment_row.get(attr_col, None)
                        if attr_value is not None:
                            if isinstance(attr_value, np.integer):
                                attr_value = int(attr_value)
                            elif isinstance(attr_value, np.floating):
                                attr_value = float(attr_value)
                            elif isinstance(attr_value, np.ndarray):
                                attr_value = attr_value.tolist()
                        attributes[attr_name] = attr_value

                    # Only add non-empty attributes
                    if any(v is not None for v in attributes.values()):
                        argument["attributes"] = attributes

        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=int(total_num),  # Convert to native int
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        # Convert potential NumPy types to native Python types
        level = (
            int(melted_label["level"]) if isinstance(melted_label["level"], int | np.integer) else melted_label["level"]
        )
        cluster_id = str(melted_label["id"])
        label = str(melted_label["label"])
        takeaway = str(melted_label["description"])
        value = (
            int(melted_label["value"]) if isinstance(melted_label["value"], int | np.integer) else melted_label["value"]
        )
        parent = str(melted_label.get("parent", "全体"))

        # Handle density_rank_percentile which might be None or a numeric value
        density_rank = melted_label.get("density_rank_percentile")
        if density_rank is not None:
            if isinstance(density_rank, float | np.floating):
                density_rank = float(density_rank)
            elif isinstance(density_rank, int | np.integer):
                density_rank = int(density_rank)

        cluster_value = Cluster(
            level=level,
            id=cluster_id,
            label=label,
            takeaway=takeaway,
            value=value,
            parent=parent,
            density_rank_percentile=density_rank,
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(PIPELINE_DIR / f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, comments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            value = row[prop] if not pd.isna(row[prop]) else None

            # Convert NumPy types to Python native types
            if value is not None:
                if isinstance(value, np.integer):
                    value = int(value)
                elif isinstance(value, np.floating):
                    value = float(value)
                elif isinstance(value, np.ndarray):
                    value = value.tolist()
                else:
                    # Convert any other types to string to ensure serialization
                    try:
                        value = str(value)
                    except Exception as e:
                        print(f"Error converting value to string: {e}")
                        value = None

            # Make sure arg_id is string
            str_arg_id = str(arg_id)
            property_map[prop][str_arg_id] = value

    return property_map
1d:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
1e:T194c,import concurrent.futures
import json
import logging
import os
import re

import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai
from services.parse_json_list import parse_extraction_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


class ExtractionResponse(BaseModel):
    extractedOpinionList: list[str] = Field(..., description="抽出した意見のリスト")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    if "provider" not in config:
        raise RuntimeError("provider is not set")
    provider = config["provider"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(
            batch_inputs, prompt, model, workers, provider, config.get("local_llm_address"), config
        )

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument = arg
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": argument,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.DEBUG)


def extract_batch(batch, prompt, model, workers, provider="openai", local_llm_address=None, config=None):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))
            for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]
        total_token_input = 0
        total_token_output = 0
        total_token_usage = 0

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and len(result) == 4:
                        items, token_input, token_output, token_total = result
                        results[i] = items
                        total_token_input += token_input
                        total_token_output += token_output
                        total_token_usage += token_total
                    else:
                        results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []

        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + total_token_usage
            config["token_usage_input"] = config.get("token_usage_input", 0) + total_token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + total_token_output
            print(
                f"Extraction batch: input={total_token_input}, output={total_token_output}, total={total_token_usage} tokens"
            )

        return results


def extract_arguments(input, prompt, model, provider="openai", local_llm_address=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            is_json=False,
            json_schema=ExtractionResponse,
            provider=provider,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )
        items = parse_extraction_response(response)
        items = list(filter(None, items))  # omit empty strings
        return items, token_input, token_output, token_total
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1f:T458,import os

import pandas as pd
from tqdm import tqdm

from services.llm import request_to_embed


def embedding(config):
    model = config["embedding"]["model"]
    is_embedded_at_local = config["is_embedded_at_local"]
    # print("start embedding")
    # print(f"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}")

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/embeddings.pkl"
    arguments = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings = []
    batch_size = 1000
    for i in tqdm(range(0, len(arguments), batch_size)):
        args = arguments["argument"].tolist()[i : i + batch_size]
        embeds = request_to_embed(
            args,
            model,
            is_embedded_at_local,
            config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )
        embeddings.extend(embeds)
    df = pd.DataFrame([{"arg-id": arguments.iloc[i]["arg-id"], "embedding": e} for i, e in enumerate(embeddings)])
    df.to_pickle(path)
20:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
21:T1c36,import json
import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    # トークン使用量を追跡するための変数を初期化
    config["total_token_usage"] = config.get("total_token_usage", 0)

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
        config["provider"],
        config.get("local_llm_address"),
        config,  # configを渡して、トークン使用量を累積できるようにする
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
        provider=provider,
        local_llm_address=local_llm_address,
        config=config,  # configを渡す
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
    provider: str = "openai",
    local_llm_address: str | None = None,
    config: dict | None = None,  # configを追加
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名
        provider: LLMプロバイダー
        local_llm_address: ローカルLLMのアドレス
        config: 設定情報を含む辞書（トークン使用量の累積に使用）

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=model,
            provider=provider,
            json_schema=LabellingFromat,
            local_llm_address=local_llm_address,
            user_api_key=os.getenv("USER_API_KEY"),
        )

        # トークン使用量を累積（configが渡されている場合）
        if config is not None:
            config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
            config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
            config["token_usage_output"] = config.get("token_usage_output", 0) + token_output

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
22:T337b,import json
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from tqdm import tqdm

from services.llm import request_to_chat_ai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
            - provider: LLMプロバイダー
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


class LabellingFromat(BaseModel):
    """ラベリング結果のフォーマットを定義する"""

    label: str = Field(..., description="クラスタのラベル名")
    description: str = Field(..., description="クラスタの説明文")


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response_text, token_input, token_output, token_total = request_to_chat_ai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            json_schema=LabellingFromat,
            provider=config["provider"],
            local_llm_address=config.get("local_llm_address"),
            user_api_key=os.getenv("USER_API_KEY"),
        )

        config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
        config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
        config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
        print(f"Merge labelling: input={token_input}, output={token_output}, total={token_total} tokens")

        response_json = json.loads(response_text) if isinstance(response_text, str) else response_text
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
23:Ta82,"""Create summaries for the clusters."""

import json
import os
import re

import pandas as pd
from pydantic import BaseModel, Field

from services.llm import request_to_chat_ai


class OverviewResponse(BaseModel):
    summary: str = Field(..., description="クラスターの全体的な要約")


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input_text = ""
    for i, _ in enumerate(ids):
        input_text += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input_text += descriptions[i] + "\n\n"

    messages = [{"role": "system", "content": prompt}, {"role": "user", "content": input_text}]
    response_text, token_input, token_output, token_total = request_to_chat_ai(
        messages=messages,
        model=model,
        provider=config["provider"],
        local_llm_address=config.get("local_llm_address"),
        user_api_key=os.getenv("USER_API_KEY"),
        json_schema=OverviewResponse,
    )

    # トークン使用量を累積
    config["total_token_usage"] = config.get("total_token_usage", 0) + token_total
    config["token_usage_input"] = config.get("token_usage_input", 0) + token_input
    config["token_usage_output"] = config.get("token_usage_output", 0) + token_output
    print(f"Hierarchical overview: input={token_input}, output={token_output}, total={token_total} tokens")

    try:
        # structured outputとしてパースできるなら処理する
        if isinstance(response_text, dict):
            parsed_response = response_text
        else:
            parsed_response = json.loads(response_text)

        with open(path, "w") as file:
            file.write(parsed_response["summary"])

    except Exception:
        # thinkタグが出力されるReasoningモデル用に、thinkタグを除去する
        thinking_removed = re.sub(
            r"<think\b[^>]*>.*?</think>",
            "",
            response_text,
            flags=re.DOTALL,
        )

        with open(path, "w") as file:
            file.write(thinking_removed)
8:[["$","$L11",null,{}],["$","$L12",null,{"className":"container","mt":"8","children":[["$","$L12",null,{"mx":"auto","maxW":"750px","mb":8,"children":[["$","$L13",null,{"textAlign":"left","fontSize":"xl","mb":5,"children":"レポート"}],["$","$L13",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"しながわ防災区民憲章を次世代に継承し、広めていくために"}],["$","$L14",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L15",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"161","件"]}],["$","p",null,{"children":"地域社会の防災意識向上には、日常生活と防災を結びつける取り組みが重要で、特に避難場所の確認や防災訓練が求められています。また、地域住民の意識を高めるためには、親しみやすい防災情報の普及や地域の伝統を尊重する意識の育成が必要です。さらに、地域と教育機関の連携を強化し、子どもたちの学びを深める環境を整えることが期待されています。体験イベントやSNSを活用した広報戦略も、地域の魅力を広めるために重要な要素です。"}]]}],["$","$L16",null,{"result":{"arguments":[{"arg_id":"Acsv-1_0","argument":"震災の節目に対する品川区の想いが理解できない","x":1.1415943,"y":4.394499,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-1_1","argument":"品川区で防災区民憲章を制定する理由がわからない","x":2.0676827,"y":6.3836455,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-1_2","argument":"時間の節目を利用されているようで良く思わない","x":4.4330525,"y":3.9250336,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-1_3","argument":"継承を考えるよりも必要性の理解を深めることを優先すべき","x":3.8698373,"y":5.365132,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-2_0","argument":"憲章は親しみやすく短いものが良いと思う","x":2.656733,"y":6.6394954,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-2_1","argument":"おしゃれなデザインで区民の目につくところにあることが重要","x":2.8352764,"y":4.7375455,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-3_0","argument":"普段目にするところに出てくると、印象に残るかもしれない。","x":3.0495045,"y":4.7496753,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-3_1","argument":"CMを作って駅前のビジョンなどで流してもらえると良い。","x":3.8693314,"y":3.5763783,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-4_0","argument":"保育園児や小学生が覚えられるレベルのわかりやすさや親しみやすさが必要である。","x":5.351694,"y":6.146519,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-4_1","argument":"内容がわかりやすく書けないのであれば、区の職員が時間を使うことは税金の無駄遣いである。","x":2.0141215,"y":6.0327835,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-5_0","argument":"初回テーマが渋いと感じる理由は、参加動機が弱く、多くの区民にとって「自分ゴト化」が難しいからである。","x":2.8054326,"y":5.3903027,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-5_1","argument":"アウトプットが遠く、憲章が抽象的で「投稿した意見がどう形になるか」がイメージしにくい。","x":2.6679435,"y":6.52233,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-5_2","argument":"デジタルの特性が活かせておらず、テーマが硬くて盛り上がりづらい。","x":3.1872675,"y":5.276977,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-5_3","argument":"堅苦しいテーマだからこそ、楽しめる体験イベントに昇華できると良い。","x":5.6817703,"y":4.3640847,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-5_4","argument":"ゲーム型のVRコンテンツを作り、小学校の特別授業として参観日などで実施するのが良い。","x":5.2774205,"y":5.6790304,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-5_5","argument":"外国人観光客に災害時に助けを求められた場合を考えさせるコンテンツが良い。","x":1.1425731,"y":3.6187296,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-5_6","argument":"生成AIの登場でコンテンツの制作コストが下がっている。","x":3.7774248,"y":6.1134686,"p":0,"cluster_ids":["0","1_5","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-6_0","argument":"先日の大雨で区内の立会川が氾濫し、戸越銀座が浸水したことについてのフィードバックが必要である。","x":2.414062,"y":3.422776,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-6_1","argument":"振り返りを広げて考えることで、次世代への継承に繋がる可能性がある。","x":4.7042294,"y":5.1808724,"p":0,"cluster_ids":["0","1_5","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-7_0","argument":"思い切って歌をつくるべきである","x":3.977291,"y":4.7715716,"p":0,"cluster_ids":["0","1_5","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-7_1","argument":"アーティストとコラボすることが重要である","x":3.522725,"y":4.5631123,"p":0,"cluster_ids":["0","1_3","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-8_0","argument":"歌は良いと思う","x":4.1941066,"y":4.848672,"p":0,"cluster_ids":["0","1_5","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-8_1","argument":"ダンスがあれば体で覚えられそう","x":2.982096,"y":2.3225672,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-9_0","argument":"次世代に引き継いでいくためにはSNSでの積極的な広報が不可欠である。","x":5.326839,"y":3.1533182,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-10_0","argument":"学校でも周知するべきである","x":4.209613,"y":5.9984407,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-11_0","argument":"PR活動としてメディアを活用すべき","x":5.0185356,"y":2.6497958,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-11_1","argument":"品川ゆかりの人や場所を活用すべき","x":2.4367843,"y":4.623365,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-11_2","argument":"駅での取組みを行うべき","x":3.8958158,"y":3.7178113,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-12_0","argument":"町会から頂いている品は「無事」である","x":1.911798,"y":4.2846413,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-12_1","argument":"まだ使用したことはないが、うれしい","x":4.265801,"y":3.7575374,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-13_0","argument":"いのちに関わることを伝えるべきである。","x":4.983739,"y":5.171223,"p":0,"cluster_ids":["0","1_5","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-14_0","argument":"本日のようなイベントは重要である。","x":5.4959884,"y":4.0794597,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-15_0","argument":"いろいろな所に貼るべきである。","x":4.5557327,"y":4.5385327,"p":0,"cluster_ids":["0","1_5","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-16_0","argument":"町会などの防災訓練に不参加の区民への働きかけが必要","x":2.1063738,"y":2.2936296,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-17_0","argument":"しながわ防災区民憲章を多くの人に知ってもらうため、チラシ等で周知すべきである。","x":2.3704014,"y":6.5238376,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-17_1","argument":"しながわ防災区民憲章の内容がなじみのない部分も多いため、より簡潔なものを広めることが望ましい。","x":2.1895483,"y":6.4416046,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-18_0","argument":"SNSやインフルエンサーを活用した広報が重要である","x":5.625454,"y":3.0576932,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-19_0","argument":"防災イベントを開催し、いろんな方に参加してもらうことが重要である。","x":1.6887898,"y":2.524594,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-19_1","argument":"学校と区が連携して授業やイベントで防災を取り入れるべきである。","x":1.632674,"y":2.8595977,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-20_0","argument":"幅広い世代で楽しめる防災訓練を実施すべき","x":1.9100808,"y":1.9362775,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-21_0","argument":"町会の行事に若い方も参加して交流を深めるべきである。","x":3.6586077,"y":4.0431857,"p":0,"cluster_ids":["0","1_3","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-22_0","argument":"デジタル技術を使うべきである","x":3.4198854,"y":4.991513,"p":0,"cluster_ids":["0","1_3","2_13"],"attributes":null,"url":null},{"arg_id":"Acsv-23_0","argument":"水の逃げ道を作るべきである。","x":2.3865395,"y":3.0160923,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-24_0","argument":"阪神・淡路大震災については生まれておらず、東日本大震災の時は関西にいたため、よくわからない。","x":1.0203273,"y":4.169585,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-24_1","argument":"この取り組み自体を知らなかった。","x":4.1323767,"y":3.9980655,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-25_0","argument":"学校や地域等で若い人がもっと興味を持つようにするべきだと思う。","x":4.8069034,"y":5.857232,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-26_0","argument":"学校で教育を行うべき","x":4.488231,"y":6.4575477,"p":0,"cluster_ids":["0","1_5","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-26_1","argument":"保護者に周知するようにしていくのが良いと思う","x":3.6122124,"y":5.561165,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-27_0","argument":"地域ごとに消防団だけでなく、やる気のある人を集めて活動できるように準備すべき","x":2.1727974,"y":1.9248819,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-27_1","argument":"役所の人ももっと積極的に活動すべき","x":3.6906524,"y":3.1037667,"p":0,"cluster_ids":["0","1_3","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-28_0","argument":"いろんな所に掲示するべきである","x":4.8145885,"y":4.6675687,"p":0,"cluster_ids":["0","1_5","2_24"],"attributes":null,"url":null},{"arg_id":"Acsv-29_0","argument":"しながわ防災区民憲章は簡潔で分かりやすく覚えやすく書く必要がある。","x":2.2349107,"y":6.4934745,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-29_1","argument":"自助、共助の在り方を次世代に伝えることが大切である。","x":4.9274874,"y":5.043123,"p":0,"cluster_ids":["0","1_5","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-29_2","argument":"公助部分は役所も被災しているため、緊急時には間に合わないことを教え伝えることが重要である。","x":1.6841857,"y":3.553897,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-30_0","argument":"新たに防災に関する場の設定や取り組みをするのは負担になるため、今行っている行事に防災に関するものを取り入れるべきである。","x":1.6723781,"y":3.0158472,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-30_1","argument":"他の福祉法人が防災訓練をお祭り前に行い、地域の方が避難した後にお祭りを開始することで成果を上げている事例がある。","x":1.8429503,"y":2.1143851,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-31_0","argument":"地域住民意識が欠落しつつあるのを止められない","x":2.3670774,"y":5.5191483,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-31_1","argument":"意識の無い新住民は地域の伝統に対して尊敬を持たず、関心を持とうとしない","x":2.237389,"y":5.5911164,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-31_2","argument":"新住民は個人主義的な気質になっている","x":2.3767283,"y":5.817155,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-32_0","argument":"現実的なトイレの問題などのリアルを訓練に絞って行うのが良いと思う。","x":2.5816617,"y":2.1929417,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-33_0","argument":"町会に加入してくれる方が増えると良いと思う","x":3.2915556,"y":3.308422,"p":0,"cluster_ids":["0","1_3","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-33_1","argument":"特にマンションの方もコミュニティは必要だと思う","x":3.3550522,"y":3.5095344,"p":0,"cluster_ids":["0","1_3","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-35_0","argument":"品川区は日頃から防災イベントが多いと感じる","x":1.5071068,"y":4.1459556,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-35_1","argument":"特に子供向けの防災イベントが多く、親としても意識することができた","x":1.1946799,"y":3.0137033,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-36_0","argument":"トイレを設置してほしい。","x":2.5426137,"y":2.8910217,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-37_0","argument":"防災訓練にはより多くの若い人達の参加を浸透させることが今後の課題である。","x":1.7935537,"y":1.9751554,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-38_0","argument":"目黒区在住なので分かりません。","x":1.88551,"y":5.2840514,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-39_0","argument":"歌をもっと誰でも歌えるように校歌のようにするべき","x":4.2152524,"y":5.377812,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-41_0","argument":"近隣の人を把握することが重要である。","x":2.8550577,"y":4.0153804,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-42_0","argument":"大きな災害の経験がないと説明はできない。","x":0.955606,"y":3.960988,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-43_0","argument":"災害だけをテーマにした取り組みではなく、日常生活やエンタメと防災を組み合わせるべきである。","x":1.046891,"y":3.3702765,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-43_1","argument":"身近なことと防災がつながっていることを伝えていく必要がある。","x":1.4967844,"y":3.4201345,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-44_0","argument":"ご近所さんとの助け合いが重要である","x":2.7346547,"y":4.0385656,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-44_1","argument":"声のかけあいが大切である","x":3.6137416,"y":4.7218146,"p":0,"cluster_ids":["0","1_3","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-45_0","argument":"SNSはイベントの告知や参加者の交流に役立つ。","x":5.844772,"y":3.3268335,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-45_1","argument":"イベントはSNSを通じて広く告知され、多くの人々を集めることができる。","x":5.776443,"y":3.49948,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-46_0","argument":"憲章についての広報誌やSNSでの情報発信を行うべき","x":5.1467505,"y":2.9927325,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-46_1","argument":"学校での授業に憲章を取り入れるべき","x":3.7582114,"y":6.524274,"p":0,"cluster_ids":["0","1_5","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-47_0","argument":"楽しいイベントやキャンペーンの意義は、実際に行って聞くことで理解できる。","x":5.868347,"y":4.27566,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-48_0","argument":"町内会活動は地域のつながりを強化するために重要である。","x":2.9772935,"y":3.7514613,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-48_1","argument":"町内会活動を通じて住民同士の交流を促進すべきである。","x":3.2866278,"y":3.6214862,"p":0,"cluster_ids":["0","1_3","2_0"],"attributes":null,"url":null},{"arg_id":"Acsv-49_0","argument":"SNSは情報を広めていくための有効な手段である。","x":5.6974273,"y":3.0959444,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-50_0","argument":"PR活動は企業やブランドの認知度を高めるために重要である。","x":5.177306,"y":2.5219643,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-50_1","argument":"PR活動はターゲットオーディエンスとの信頼関係を築く手段である。","x":5.3042436,"y":2.6782048,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-51_0","argument":"イベント等での発信が重要である","x":5.528588,"y":3.6039038,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-52_0","argument":"しながわの良さを区民憲章にのせるべきである。","x":2.4603992,"y":6.283289,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-52_1","argument":"公的なイベントや学校の行事を通じてしながわの良さに接する機会を増やすべきである。","x":5.260793,"y":4.4670906,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-53_0","argument":"憲章を周知すべきである。","x":3.0663662,"y":6.623125,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-54_0","argument":"人同士の声がけは重要である。","x":3.368436,"y":4.336433,"p":0,"cluster_ids":["0","1_3","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-55_0","argument":"イベントやゲーム等で子供達にも伝えていけることはよいと思います。","x":5.57214,"y":5.4138613,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-56_0","argument":"今回のような取り組みはとてもよいと思います。","x":4.539226,"y":3.950379,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-57_0","argument":"触れる機会を増やすべきだと思います。","x":4.124057,"y":2.903951,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-58_0","argument":"学校でお便りをもらうことで、子供の意識が高まり良いと思う","x":5.1521735,"y":5.9061017,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-59_0","argument":"学校の授業にもAIを取り入れるべきである。","x":4.0583134,"y":6.220614,"p":0,"cluster_ids":["0","1_5","2_16"],"attributes":null,"url":null},{"arg_id":"Acsv-60_0","argument":"防災食を小学校で作ったり食べたりすることで、皆が身近に感じることができる。","x":0.88734573,"y":2.9395108,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-60_1","argument":"防災食を通じて安心感を得ることができる。","x":0.9105076,"y":3.2669523,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-61_0","argument":"防災トイレが送られてきて助かる","x":1.4430807,"y":3.450561,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-62_0","argument":"今のままでOK","x":4.477535,"y":3.5992668,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-63_0","argument":"家族が会社や学校などバラバラになっている時にどう工夫すればよいか悩む","x":4.541347,"y":6.014161,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-64_0","argument":"地域での防災に関する確認のための集まりが必要である。","x":1.9145272,"y":2.5079794,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-65_0","argument":"防災訓練で若い人の参加が少ないかもしれないので、広報が大事である。","x":2.008413,"y":2.1921916,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-66_0","argument":"インスタ等を活用するべきである","x":4.9197736,"y":3.4573927,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-67_0","argument":"子どもも楽しんで一緒に考えられるイベントがもっと増えるべきである","x":5.7118053,"y":5.0468016,"p":0,"cluster_ids":["0","1_5","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-67_1","argument":"そのようなイベントは参加しやすく、防災意識も高まりやすくなる","x":1.4038702,"y":2.47443,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-68_0","argument":"子供たちに分かりやすい動画を作成し、小学校の授業で理解を深めるべきである。","x":5.206081,"y":5.955943,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-69_0","argument":"もう少し露出を増やしてほしい","x":4.1464024,"y":3.065097,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-70_0","argument":"楽しみながら学べるのが良いと思う","x":4.706028,"y":5.526546,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-72_0","argument":"避難訓練などがあったら積極的に参加するべきである。","x":2.5050359,"y":2.0045376,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-73_0","argument":"避難場所、経路を家族でしっかり話し合っておくべきである。","x":2.1160629,"y":3.1985219,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-73_1","argument":"特に小さい子供がいるので、連携をきちんとする必要がある。","x":3.3302345,"y":4.157044,"p":0,"cluster_ids":["0","1_3","2_1"],"attributes":null,"url":null},{"arg_id":"Acsv-74_0","argument":"イベント・訓練への参加者を増やすべき","x":3.28742,"y":2.0908616,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-75_0","argument":"地元の連携が重要である","x":3.1407692,"y":4.0338063,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-76_0","argument":"このイベントは特別な意味を持つ。","x":5.6107664,"y":4.0544405,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-77_0","argument":"SNSや学校での周知があると良い。","x":4.383527,"y":5.784196,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-78_0","argument":"定期的に防災用具の配布を行うべきである。","x":1.4360454,"y":3.0173616,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-78_1","argument":"防災意識を確認する機会を創設すべきである。","x":1.0887097,"y":2.8527944,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-79_0","argument":"親子で防災について学ぶ良いきっかけになった","x":1.1692371,"y":2.7876437,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-79_1","argument":"今後もこのような取り組みを続けてほしい","x":4.2853184,"y":3.4028587,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-80_0","argument":"子供はゲームが好きなので、遊びながら知識を得られる類のものが取り組みやすいと思う。","x":5.4724717,"y":5.7352734,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-81_0","argument":"浸水時などの復旧時に補助が必要である","x":2.4068263,"y":3.6506448,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-81_1","argument":"復旧時のための連絡先や備品が重要である","x":2.726055,"y":4.215512,"p":0,"cluster_ids":["0","1_3","2_18"],"attributes":null,"url":null},{"arg_id":"Acsv-82_0","argument":"戸越銀座商店街の浸水問題に対する改善策が必要である。","x":2.4518564,"y":3.2595835,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-83_0","argument":"防災ポータルや防災アプリで情報を確認できると良いと思う","x":0.82449144,"y":3.2027917,"p":0,"cluster_ids":["0","1_1","2_10"],"attributes":null,"url":null},{"arg_id":"Acsv-83_1","argument":"防災に対する意識を身近に感じられることが重要だと思う","x":1.0195892,"y":3.4277663,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-84_0","argument":"学校で子どもたちに何かを伝えるべきである。","x":5.284735,"y":5.9586844,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-85_0","argument":"広報活動の回数を増やすべきである","x":4.5030556,"y":2.73289,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-86_0","argument":"防災イベントに有名人を呼ぶことは、普段参加しない人を呼ぶ点では良いアイデアだと思う。","x":1.2811918,"y":2.4136174,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-87_0","argument":"SNSや有名人に宣伝してもらうべき","x":5.571375,"y":2.8174849,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-87_1","argument":"使う人を増やす必要がある","x":4.049578,"y":2.6515698,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-88_0","argument":"本日のようなイベントが必要である","x":5.4051533,"y":4.037926,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-88_1","argument":"SNSが必要である","x":5.8043056,"y":3.0639834,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-89_0","argument":"今回のようなイベントは効果的と思います。","x":5.0073013,"y":4.0256433,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-90_0","argument":"防災訓練に参加するべきである。","x":1.6855989,"y":1.8718989,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-91_0","argument":"ホームページやSNSでのPRを行うべき","x":5.223841,"y":2.764642,"p":0,"cluster_ids":["0","1_2","2_9"],"attributes":null,"url":null},{"arg_id":"Acsv-91_1","argument":"家へのチラシの配布を行うべき","x":4.6139736,"y":2.9495656,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-92_0","argument":"特に行っていないが、これから活動しようと思う。","x":4.2306457,"y":3.5270612,"p":0,"cluster_ids":["0","1_3","2_8"],"attributes":null,"url":null},{"arg_id":"Acsv-93_0","argument":"避難場所は災害時に人々が安全に避難できる場所として重要である。","x":1.9129194,"y":3.150538,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-94_0","argument":"子供向けのイベントを行うべきである。","x":5.614383,"y":4.968445,"p":0,"cluster_ids":["0","1_5","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-95_0","argument":"家に防災かばんを置いているべきである","x":1.6597284,"y":3.1219163,"p":0,"cluster_ids":["0","1_1","2_22"],"attributes":null,"url":null},{"arg_id":"Acsv-96_0","argument":"もっと周知する必要がある","x":3.8129923,"y":5.732708,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-97_0","argument":"今回のような体験会を開催するべきである。","x":5.2410536,"y":4.1271644,"p":0,"cluster_ids":["0","1_2","2_21"],"attributes":null,"url":null},{"arg_id":"Acsv-98_0","argument":"コミュニケーションの場を増やすべきである。","x":4.293782,"y":2.8852887,"p":0,"cluster_ids":["0","1_3","2_23"],"attributes":null,"url":null},{"arg_id":"Acsv-99_0","argument":"イベントは人々が集まり、交流する機会を提供する重要な場である。","x":5.4892654,"y":3.8754237,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-99_1","argument":"イベントの企画や運営には、参加者のニーズを考慮することが不可欠である。","x":5.7044444,"y":3.7420878,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-100_0","argument":"分からない","x":1.9288218,"y":5.345031,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-101_0","argument":"防災区民憲章を短冊にしてストラップ等にし、イベントで配布すべき","x":2.2498376,"y":6.6304846,"p":0,"cluster_ids":["0","1_4","2_5"],"attributes":null,"url":null},{"arg_id":"Acsv-102_0","argument":"世代を超えたイベントで必要なことを伝えて広めるべきである。","x":5.3391275,"y":4.8569784,"p":0,"cluster_ids":["0","1_5","2_11"],"attributes":null,"url":null},{"arg_id":"Acsv-103_0","argument":"まわりの人々がよくわからない。","x":1.9454571,"y":5.568458,"p":0,"cluster_ids":["0","1_4","2_12"],"attributes":null,"url":null},{"arg_id":"Acsv-104_0","argument":"防災訓練は重要である。","x":2.3260767,"y":2.1191394,"p":0,"cluster_ids":["0","1_1","2_6"],"attributes":null,"url":null},{"arg_id":"Acsv-105_0","argument":"災害がある人やボランティアの声を聞くことが大事である","x":1.2150633,"y":3.4602134,"p":0,"cluster_ids":["0","1_1","2_14"],"attributes":null,"url":null},{"arg_id":"Acsv-105_1","argument":"学校でいろんな意見を探るべきである","x":4.6143346,"y":6.053892,"p":0,"cluster_ids":["0","1_5","2_4"],"attributes":null,"url":null},{"arg_id":"Acsv-106_0","argument":"学校での訓練が重要である","x":2.8771937,"y":2.1891768,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-106_1","argument":"地域での訓練も重要である","x":2.553436,"y":2.0703952,"p":0,"cluster_ids":["0","1_1","2_19"],"attributes":null,"url":null},{"arg_id":"Acsv-106_2","argument":"子どもは掲示板などを見ることがある","x":5.6259875,"y":5.6363983,"p":0,"cluster_ids":["0","1_5","2_15"],"attributes":null,"url":null},{"arg_id":"Acsv-107_0","argument":"区民まつりでの周知が効果的である","x":3.275586,"y":5.6404953,"p":0,"cluster_ids":["0","1_5","2_20"],"attributes":null,"url":null},{"arg_id":"Acsv-108_0","argument":"東京都はお金をかけているので意外と安心である","x":1.6959928,"y":4.3889775,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-108_1","argument":"品川区は特に安心である","x":1.8689829,"y":4.5606046,"p":0,"cluster_ids":["0","1_1","2_7"],"attributes":null,"url":null},{"arg_id":"Acsv-109_0","argument":"町内会や防災イベントに行きやすくするための施策が必要である。","x":2.119439,"y":3.1057599,"p":0,"cluster_ids":["0","1_1","2_3"],"attributes":null,"url":null},{"arg_id":"Acsv-109_1","argument":"地域のイベントについての情報を知ることが重要である。","x":5.8986387,"y":4.0139103,"p":0,"cluster_ids":["0","1_2","2_2"],"attributes":null,"url":null},{"arg_id":"Acsv-110_0","argument":"教育は個人の成長や社会の発展に不可欠である。","x":4.700948,"y":6.5544324,"p":0,"cluster_ids":["0","1_5","2_17"],"attributes":null,"url":null},{"arg_id":"Acsv-110_1","argument":"教育の質を向上させるためには、教員の研修や教育制度の見直しが必要である。","x":4.710164,"y":6.5803266,"p":0,"cluster_ids":["0","1_5","2_17"],"attributes":null,"url":null}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":161,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_1","label":"地域社会の防災意識向上と実践的な取り組みの強化","takeaway":"地域住民の防災意識を高めるためには、日常生活やエンターテインメントと防災を結びつけることが重要です。特に、避難場所の確認や地域での防災訓練の実施、学校との連携による防災教育が求められています。また、親子で参加できる防災イベントや、幅広い世代が楽しめる訓練の実施が、地域全体の防災力を向上させる鍵となります。さらに、外国人観光客への配慮や、実際の災害経験を通じた学びの重要性も強調されており、地域全体での協力が不可欠です。","value":50,"parent":"0","density_rank_percentile":0.8},{"level":1,"id":"1_4","label":"地域住民の意識向上と防災情報の効果的な普及","takeaway":"地域住民の防災意識を高めるためには、品川区の防災区民憲章を親しみやすく具体的に普及させることが重要です。具体的な手法として、簡潔で覚えやすい内容のチラシや短冊形式での配布が提案されており、これにより区民が憲章の内容を理解しやすくなります。また、新住民の個人主義的な傾向や地域住民意識の低下が懸念されており、地域の伝統や文化を尊重する意識を育むことも求められています。これらの取り組みを通じて、地域の結束を強化し、災害時の対応力を向上させることが期待されています。","value":16,"parent":"0","density_rank_percentile":0.2},{"level":1,"id":"1_3","label":"地域住民の参加を促進するためのプロモーションとコミュニティ強化","takeaway":"地域の駅前でのプロモーション活動やデジタル技術を活用した魅力的なテーマ設定を通じて、住民の参加意欲を高める取り組みが求められています。また、地域コミュニティの連携や助け合いを強化するために、町内会活動や役所の積極的な関与が重要視されています。具体的には、広報活動の頻度を上げたり、住民同士の交流を促進するイベントを開催することで、地域全体のつながりを深めることが期待されています。","value":36,"parent":"0","density_rank_percentile":1},{"level":1,"id":"1_5","label":"地域と教育の連携による子どもたちの学びの深化","takeaway":"地域コミュニティと教育機関が連携し、子どもたちが楽しみながら学べる環境を整えることが求められています。具体的には、ゲーム型の教育コンテンツや世代を超えたイベントを通じて、子どもたちに必要な知識や価値観を伝えることが重要視されています。また、保護者への情報提供や地域活動への参加促進を通じて、地域全体で子どもたちの成長を支える体制の構築が期待されています。教育の質向上に向けた制度改革やAIの活用も重要な要素として挙げられ、次世代への知識の継承が強調されています。","value":35,"parent":"0","density_rank_percentile":0.6},{"level":1,"id":"1_2","label":"地域の魅力を引き出す体験イベントとSNS活用による広報戦略","takeaway":"しながわの魅力を体験するイベントの開催は、地域の良さを広めるために不可欠です。公的なイベントや学校行事を通じて、参加者が楽しみながらしながわの魅力を実感できる機会を増やすことが求められています。また、SNSを活用したPR活動は、ターゲットオーディエンスとの信頼関係を築き、地域イベントへの参加を促進するための重要な手段です。次世代への情報発信を考慮し、SNSやインフルエンサーを通じて広報を行うことで、地域のイベントがより多くの人々に認知され、参加者同士の交流を深めることが期待されています。","value":24,"parent":"0","density_rank_percentile":0.4},{"level":2,"id":"2_7","label":"品川区の防災意識と震災への理解のギャップ","takeaway":"この意見グループは、震災に対する品川区の防災意識や取り組みについての評価と、個々の震災経験の違いから生じる理解のギャップに焦点を当てています。特に、品川区の防災イベントの多さや東京都の資金投入による安心感が強調される一方で、震災を直接経験していない人々の理解が不足していることが示唆されています。","value":6,"parent":"1_1","density_rank_percentile":0.96},{"level":2,"id":"2_5","label":"親しみやすく具体的な防災区民憲章の普及","takeaway":"この意見グループは、品川区の防災区民憲章をより多くの区民に理解してもらうために、簡潔で覚えやすい内容にすることや、親しみやすい形での周知方法を提案しています。具体的には、チラシや短冊形式での配布を通じて、憲章の内容を広めることが重要であると考えられています。また、憲章の内容が抽象的であることへの懸念も示されており、具体的な形でのアウトプットが求められています。","value":9,"parent":"1_4","density_rank_percentile":0.52},{"level":2,"id":"2_8","label":"駅前でのプロモーション活動と市民参加の促進","takeaway":"この意見グループは、駅前でのCMやビジョンを活用したプロモーション活動の重要性や、地域住民の参加を促す取り組みについての意見が中心です。特に、駅での活動やSNSの活用に対する期待が表れています。また、取り組みの認知度や今後の継続的な活動への期待も含まれています。","value":10,"parent":"1_3","density_rank_percentile":0.84},{"level":2,"id":"2_20","label":"地域コミュニティの周知と参加促進","takeaway":"この意見グループは、区民まつりを通じた地域の周知活動の重要性や、校歌のように親しみやすい歌の普及、保護者への情報提供の必要性、そして地域活動への理解を深めることの優先順位についての意見が中心です。地域コミュニティの活性化を目指すための周知活動の強化が求められています。","value":5,"parent":"1_5","density_rank_percentile":0.76},{"level":2,"id":"2_13","label":"区民参加を促進するための魅力的なテーマ設定とデジタル活用","takeaway":"この意見グループは、初回テーマが渋いと感じる理由として、参加動機の弱さや「自分ゴト化」の難しさを挙げています。また、区民の目に留まるおしゃれなデザインや、デジタル技術の活用が重要であると強調されており、普段目にする場所での印象付けが参加意欲を高める可能性についても言及されています。全体として、テーマの魅力を高めることで区民の関心を引き、参加を促すことが求められています。","value":5,"parent":"1_3","density_rank_percentile":0.92},{"level":2,"id":"2_15","label":"子ども向け教育コンテンツのゲーム化と親しみやすさ","takeaway":"この意見グループは、子どもたちに効果的に情報を伝えるための方法として、ゲーム型のVRコンテンツや分かりやすい動画の作成を提案しています。特に、子どもたちが興味を持ちやすい遊びを通じて学ぶことが重要視されており、親しみやすさや理解しやすさが求められています。学校や保育園での実施を通じて、子どもたちの意識を高めることが期待されています。","value":8,"parent":"1_5","density_rank_percentile":0.28},{"level":2,"id":"2_12","label":"地域住民意識の低下と新住民の個人主義","takeaway":"この意見グループは、地域住民の意識が低下していることや、新住民が地域の伝統や文化に対して無関心であることに焦点を当てています。また、新住民の個人主義的な傾向が地域社会に与える影響についても言及されており、地域の結束や伝統の尊重が失われつつある現状が懸念されています。","value":7,"parent":"1_4","density_rank_percentile":0.64},{"level":2,"id":"2_21","label":"しながわの魅力を体験するイベントの重要性","takeaway":"この意見グループは、しながわの良さを広めるために公的なイベントや学校行事を通じて体験の機会を増やすことの重要性を強調しています。堅苦しいテーマを楽しめる体験イベントに昇華させることで、参加者がしながわの魅力を実感できるようにすることが求められています。","value":6,"parent":"1_2","density_rank_percentile":0.32},{"level":2,"id":"2_14","label":"日常生活と防災意識の融合","takeaway":"この意見グループは、防災に対する意識を高めるためには、日常生活やエンターテインメントと防災を結びつけることが重要であるという考えが中心です。また、災害時の公助の限界や、外国人観光客への配慮、実際の災害経験を通じた学びの重要性が強調されています。身近な事例を通じて防災を意識することが、より効果的な防災対策につながるという視点が示されています。","value":8,"parent":"1_1","density_rank_percentile":0.44},{"level":2,"id":"2_16","label":"教育現場におけるAI活用によるコスト削減と授業改革","takeaway":"この意見グループは、生成AIの導入によってコンテンツ制作のコストが低下し、さらに学校教育においてもAIを活用することで授業の質を向上させるべきだという考えが中心です。AIの活用により、教育現場での効率化や新しい学びの形が期待されています。","value":3,"parent":"1_5","density_rank_percentile":0.08},{"level":2,"id":"2_3","label":"災害時の避難場所と地域防災対策の強化","takeaway":"この意見グループは、災害時における避難場所の重要性や、実際の浸水問題に対する具体的な改善策、地域住民の防災意識の向上に関する提案が中心です。特に、避難経路の確認や水の逃げ道の確保、トイレの設置など、実用的な対策が求められています。また、町内会や防災イベントへの参加促進も重要なテーマとして挙げられています。","value":8,"parent":"1_1","density_rank_percentile":0.56},{"level":2,"id":"2_24","label":"情報共有と次世代への継承の重要性","takeaway":"この意見グループは、情報やメッセージを広く掲示することの重要性に焦点を当てています。特に、振り返りを通じて得られた知見を次世代に継承するための手段として、掲示や歌の創作が提案されています。多様な方法での情報発信が、より効果的な伝達を促進する可能性があることが示唆されています。","value":5,"parent":"1_5","density_rank_percentile":1},{"level":2,"id":"2_1","label":"地域コミュニティの連携と交流促進","takeaway":"この意見グループは、地域社会における人々の声のかけあいや連携の重要性を強調しています。特に小さな子供を持つ家庭においては、コミュニケーションが円滑に行われることが求められ、町会の行事への参加を通じて世代間の交流を深める必要性が示されています。また、アーティストとのコラボレーションも地域の活性化に寄与する要素として挙げられています。","value":5,"parent":"1_3","density_rank_percentile":0.4},{"level":2,"id":"2_19","label":"地域社会における防災訓練の重要性と参加促進","takeaway":"この意見グループは、避難訓練や地域での防災訓練の重要性を強調し、現実的な問題に基づいた訓練の実施や参加者の増加を求める声が中心です。また、学校での訓練や体を使った学習方法（ダンスなど）を通じて、より効果的な防災意識の向上を図ることが提案されています。","value":6,"parent":"1_1","density_rank_percentile":0.48},{"level":2,"id":"2_9","label":"SNSを活用したPR活動の重要性と戦略","takeaway":"この意見グループは、PR活動におけるSNSの役割とその重要性に焦点を当てています。ターゲットオーディエンスとの信頼関係を築くために、SNSやインフルエンサーを活用することが不可欠であり、企業やブランドの認知度を高めるための戦略として、広報誌やメディアの活用も提案されています。次世代への情報発信を考慮した場合、SNSは特に重要な手段であると強調されています。","value":10,"parent":"1_2","density_rank_percentile":0.8},{"level":2,"id":"2_4","label":"学校と地域での意見共有と若者の関心喚起","takeaway":"この意見グループは、学校や地域において多様な意見を探求し、若者が興味を持つような工夫や周知の重要性を強調しています。特に、楽しみながら学ぶ環境を整えることや、SNSを活用した情報発信が効果的であると考えられています。","value":6,"parent":"1_5","density_rank_percentile":0.36},{"level":2,"id":"2_18","label":"地域コミュニティの強化と助け合いの重要性","takeaway":"この意見グループは、地域住民同士の助け合いや町内会活動を通じて、地域のつながりを強化することの重要性に焦点を当てています。また、近隣住民の把握や地元の資源の活用、災害時の連絡体制の整備など、地域コミュニティの連携を深めるための具体的な取り組みが求められています。","value":6,"parent":"1_3","density_rank_percentile":0.6},{"level":2,"id":"2_11","label":"世代を超えた子ども向け教育イベントの推進","takeaway":"この意見グループは、子どもたちが楽しみながら学べるイベントの重要性を強調しており、特にいのちに関わるテーマや自助・共助の概念を次世代に伝えることの必要性が中心です。また、世代を超えた交流を通じて、必要な知識や価値観を広めることが求められています。","value":5,"parent":"1_5","density_rank_percentile":0.88},{"level":2,"id":"2_6","label":"地域防災訓練の重要性と参加促進の必要性","takeaway":"この意見グループは、地域における防災訓練の重要性を強調し、特に若い世代の参加を促進するための広報活動や、地域住民の意識向上に向けた取り組みの必要性について述べています。また、地域の特性に応じた訓練の実施や、参加者を増やすための工夫が求められています。","value":9,"parent":"1_1","density_rank_percentile":0.24},{"level":2,"id":"2_22","label":"学校と地域連携による防災意識の向上と参加促進","takeaway":"この意見グループは、学校と地域が協力して防災教育やイベントを実施することの重要性を強調しています。参加しやすいイベントの開催や有名人の招致によって、防災意識を高めることができるとともに、既存の行事に防災要素を組み込むことで負担を軽減し、より多くの人々に参加を促すことができるという点が中心です。","value":7,"parent":"1_1","density_rank_percentile":0.68},{"level":2,"id":"2_17","label":"教育の質向上と社会発展のための制度改革","takeaway":"この意見グループは、教育が個人の成長や社会の発展において重要な役割を果たすことを強調しています。また、教育の質を向上させるためには、教員の研修や教育制度の見直しが必要であるという具体的な提案が含まれており、教育の改善に向けた前向きな姿勢が示されています。","value":3,"parent":"1_5","density_rank_percentile":0.04},{"level":2,"id":"2_0","label":"地域コミュニティの活性化と役所の積極的な関与","takeaway":"この意見グループは、役所が地域活動にもっと積極的に関与し、町会への加入促進や住民同士の交流を促すことの重要性を強調しています。特にマンション住民に対してもコミュニティの必要性が訴えられており、地域全体のつながりを強化するための具体的なアクションが求められています。","value":4,"parent":"1_3","density_rank_percentile":0.2},{"level":2,"id":"2_10","label":"防災意識の向上と親子の学びの場","takeaway":"この意見グループは、防災食や防災イベントを通じて、親子が共に防災について学び、意識を高めることができる点に焦点を当てています。特に子供向けの活動が多く、家庭内での防災意識の醸成や、地域での情報共有の重要性が強調されています。","value":6,"parent":"1_1","density_rank_percentile":0.12},{"level":2,"id":"2_2","label":"地域イベントの重要性とSNS活用による参加促進","takeaway":"この意見グループは、地域イベントやキャンペーンの意義を強調し、実際に参加することでその価値を理解できること、またSNSを通じての告知が多くの人々を集めるために重要である点に焦点を当てています。イベントは人々の交流の場を提供し、参加者のニーズを考慮した企画運営が求められることも示されています。","value":8,"parent":"1_2","density_rank_percentile":0.72},{"level":2,"id":"2_23","label":"露出と利用促進のための広報活動強化","takeaway":"この意見グループは、製品やサービスの認知度を高め、利用者を増やすために、広報活動やコミュニケーションの機会を増やす必要性を強調しています。具体的には、チラシ配布や広報活動の頻度を上げることが提案されており、より多くの人々に触れてもらうための戦略が求められています。","value":6,"parent":"1_3","density_rank_percentile":0.16}],"comments":{},"propertyMap":{},"translations":{},"overview":"地域社会の防災意識向上には、日常生活と防災を結びつける取り組みが重要で、特に避難場所の確認や防災訓練が求められています。また、地域住民の意識を高めるためには、親しみやすい防災情報の普及や地域の伝統を尊重する意識の育成が必要です。さらに、地域と教育機関の連携を強化し、子どもたちの学びを深める環境を整えることが期待されています。体験イベントやSNSを活用した広報戦略も、地域の魅力を広めるために重要な要素です。","config":{"name":"shinagawa-charter-q3","input":"shinagawa-charter-q3","question":"しながわ防災区民憲章を次世代に継承し、広めていくために","intro":"しながわ防災区民憲章に対する意見募集結果です。\n分析対象となったデータの件数は110件で、これらのデータに対してOpenAI API (gpt-4o-mini)を用いて161件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","provider":"openai","is_pubcom":true,"is_embedded_at_local":false,"local_llm_address":null,"extraction":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":110,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$17","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[5,25],"source_code":"$18"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$1a","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1c"},"enable_source_link":false,"output_dir":"shinagawa-charter-q3","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"$1d"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2025-11-14T04:00:02.276700","completed_jobs":[{"step":"extraction","completed":"2025-11-14T04:00:17.962715","duration":15.683164,"params":{"prompt":"あなたは専門的なリサーチアシスタントです。与えられたテキストから、意見を抽出して整理してください。\n\n# 指示\n* 入出力の例に記載したような形式で文字列のリストを返してください\n  * 必要な場合は2つの別個の意見に分割してください。多くの場合は1つの議論にまとめる方が望ましいです。\n* 整理した意見は日本語で出力してください\n\n## 入出力の例\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\"\n  ]\n}\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、市民を教育する必要がある。また、教育できる人材を養成する必要がある。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIの能力、限界、倫理的考慮事項について、市民を教育すべき\",\n    \"AIに関する教育をできる人材を養成すべき\"\n  ]\n}\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AIはエネルギーグリッドを最適化して炭素排出を削減できる\"\n  ]\n}\n","workers":30,"limit":110,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$1e","model":"gpt-4o-mini"},"token_usage":61810},{"step":"embedding","completed":"2025-11-14T04:00:19.715666","duration":1.750061,"params":{"model":"text-embedding-3-small","source_code":"$1f"},"token_usage":0},{"step":"hierarchical_clustering","completed":"2025-11-14T04:00:26.915404","duration":7.198134,"params":{"cluster_nums":[5,25],"source_code":"$20"},"token_usage":0},{"step":"hierarchical_initial_labelling","completed":"2025-11-14T04:00:33.002800","duration":6.085644,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説し、表札（label）をつけてください。\n表札については、グループ内の具体的な論点や特徴を反映した、具体性の高い名称を考案してください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例\n- 手作業での意見分析は時間がかかりすぎる。AIで効率化できると嬉しい\n- 今のやり方だと分析に工数がかかりすぎるけど、AIならコストをかけずに分析できそう\n- AIが自動で意見を整理してくれると楽になって嬉しい\n\n\n## 出力例\n{\n    \"label\": \"AIによる業務効率の大幅向上とコスト効率化\",\n    \"description\": \"この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$21","model":"gpt-4o-mini"},"token_usage":19880},{"step":"hierarchical_merge_labelling","completed":"2025-11-14T04:00:38.105001","duration":5.09873,"params":{"prompt":"あなたはデータ分析のエキスパートです。\n現在、テキストデータの階層クラスタリングを行っています。\n下層のクラスタ（意見グループ）のタイトルと説明、およびそれらのクラスタが所属する上層のクラスタのテキストのサンプルを与えるので、上層のクラスタのタイトルと説明を作成してください。\n\n# 指示\n- 統合後のクラスタ名は、統合前のクラスタ名称をそのまま引用せず、内容に基づいた新たな名称にしてください。\n- タイトルには、具体的な事象・行動（例：地域ごとの迅速対応、復興計画の着実な進展、効果的な情報共有・地域協力など）を含めてください\n  - 可能な限り具体的な表現を用いるようにし、抽象的な表現は避けてください\n    - 「多様な意見」などの抽象的な表現は避けてください\n- 出力例に示したJSON形式で出力してください\n\n\n# サンプルの入出力\n## 入力例\n- 「顧客フィードバックの自動集約」: この意見グループは、SNSやオンラインレビューなどから集めた大量の意見をAIが瞬時に解析し、企業が市場のトレンドや顧客の要望を即時に把握できる点についての期待を示しています。\n- 「AIによる業務効率の大幅向上とコスト効率化」: この意見グループは、従来の手作業による意見分析と比較して、AIによる自動化で分析プロセスが効率化され、作業時間の短縮や運用コストの効率化が実現される点に対する前向きな評価が中心です。\n\n## 出力例\n{\n    \"label\": \"AI技術の導入による意見分析の効率化への期待\",\n    \"description\": \"大量の意見やフィードバックから迅速に洞察を抽出できるため、企業や自治体が消費者や市民の声を的確に把握し、戦略的な意思決定やサービス改善が可能になります。また、従来の手法と比べて作業負荷が軽減され、業務効率の向上やコスト削減といった実際の便益が得られると期待されています。\"\n}\n","sampling_num":30,"workers":30,"source_code":"$22","model":"gpt-4o-mini"},"token_usage":10770},{"step":"hierarchical_overview","completed":"2025-11-14T04:00:43.158937","duration":5.052024,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢の意見グループを分析し始めています。\nこれから意見グループのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$23","model":"gpt-4o-mini"},"token_usage":1421}],"total_token_usage":93881,"token_usage_input":85058,"token_usage_output":8823,"lock_until":"2025-11-14T04:05:43.161466","current_job":"hierarchical_aggregation","current_job_started":"2025-11-14T04:00:43.161458","estimated_cost":0.0180525,"current_job_progress":null,"current_jop_tasks":null},"comment_num":110,"visibility":"public"}}],["$","$L24",null,{"result":"$8:1:props:children:1:props:result"}],["$","$L12",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}],["$","$L25",null,{"my":12,"maxW":"750px","mx":"auto"}],["$","$L12",null,{"maxW":"750px","mx":"auto","mb":24,"children":"$L26"}]]}],["$","$L27",null,{"meta":{"reporter":"一般社団法人コード・フォー・ジャパン","message":"ともに考え、ともにつくる社会をビジョンに、Decidimの日本での活用などデジタル民主主義を推進しています。","webLink":"https://www.code4japan.org/","privacyLink":"https://www.code4japan.org/privacy-policy","termsLink":null,"brandColor":"#2577B1","isDefault":false}}]]
b:null
f:[["$","title","0",{"children":"しながわ防災区民憲章を次世代に継承し、広めていくために - 一般社団法人コード・フォー・ジャパン"}],["$","meta","1",{"name":"description","content":"地域社会の防災意識向上には、日常生活と防災を結びつける取り組みが重要で、特に避難場所の確認や防災訓練が求められています。また、地域住民の意識を高めるためには、親しみやすい防災情報の普及や地域の伝統を尊重する意識の育成が必要です。さらに、地域と教育機関の連携を強化し、子どもたちの学びを深める環境を整えることが期待されています。体験イベントやSNSを活用した広報戦略も、地域の魅力を広めるために重要な要素です。"}],["$","meta","2",{"property":"og:title","content":"しながわ防災区民憲章を次世代に継承し、広めていくために - 一般社団法人コード・フォー・ジャパン"}],["$","meta","3",{"property":"og:description","content":"地域社会の防災意識向上には、日常生活と防災を結びつける取り組みが重要で、特に避難場所の確認や防災訓練が求められています。また、地域住民の意識を高めるためには、親しみやすい防災情報の普及や地域の伝統を尊重する意識の育成が必要です。さらに、地域と教育機関の連携を強化し、子どもたちの学びを深める環境を整えることが期待されています。体験イベントやSNSを活用した広報戦略も、地域の魅力を広めるために重要な要素です。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/shinagawa-charter-q3/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"しながわ防災区民憲章を次世代に継承し、広めていくために - 一般社団法人コード・フォー・ジャパン"}],["$","meta","7",{"name":"twitter:description","content":"地域社会の防災意識向上には、日常生活と防災を結びつける取り組みが重要で、特に避難場所の確認や防災訓練が求められています。また、地域住民の意識を高めるためには、親しみやすい防災情報の普及や地域の伝統を尊重する意識の育成が必要です。さらに、地域と教育機関の連携を強化し、子どもたちの学びを深める環境を整えることが期待されています。体験イベントやSNSを活用した広報戦略も、地域の魅力を広めるために重要な要素です。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/shinagawa-charter-q3/opengraph-image.png"}]]
28:I[81499,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"ReporterContent"]
26:["$","$L28",null,{"meta":"$8:2:props:meta","children":"$L29"}]
2a:I[89248,["150","static/chunks/59650de3-481cc0c44db376d7.js","315","static/chunks/315-19ca0205aa2fdd55.js","567","static/chunks/567-13e48369edb584ec.js","458","static/chunks/458-4db2abcd82f4c9c6.js","874","static/chunks/874-e31c245f344a6bb3.js","261","static/chunks/261-dd24fe22c839e0ae.js","124","static/chunks/124-8b6d8ff5759d10e8.js","116","static/chunks/116-746bc821fa46af6a.js","49","static/chunks/49-b1e59d6cd5c08cd8.js","347","static/chunks/347-1fc22e09a7c5e141.js","182","static/chunks/app/%5Bslug%5D/page-a97111f7f2c073a4.js"],"Image"]
29:["$","$L2a",null,{"src":"/meta/reporter.png","alt":"一般社団法人コード・フォー・ジャパン","maxW":"150px"}]
